# 《信息传:决定我们未来发展的方法论》深度读书笔记

**一句话概括：这是一本从信息论角度重新理解世界运作规律的书。吴军用通俗的语言和丰富的案例，把香农信息论从学术殿堂拉到了你的日常生活中。他想告诉你的是：无论是技术进步、商业竞争，还是个人成长，背后都遵循着信息增益的底层逻辑。**

## 一个被低估的革命：香农到底发现了什么

你可能听说过香农，但未必真正理解他做了什么。1948年，香农发表了一篇论文《通信的数学理论》，这篇不到50页的文章彻底改变了人类对信息的认知。在此之前，人们谈论"信息"时含混不清——它可能是一条新闻，一个数据，或是某种知识。香农做的事情相当于给"信息"下了一个精确的定义：**信息就是消除不确定性**。

**这个定义的威力在于，它把抽象的"信息"变成了可以测量、可以计算的东西**。想象你在猜一个0到100之间的数字，每猜一次，对方告诉你"大了"或"小了"。每一次回答都在减少可能性的范围，这就是信息在起作用。香农用数学证明了：传递同样的信息量，需要多少比特，这是有理论极限的。这个极限叫"香农极限"，它就像物理学中的光速——你可以无限接近，但永远无法超越。

吴军在书中反复强调一点：信息论不只是关于通信的理论，它是关于"如何高效处理不确定性"的方法论。在5G通信、数据压缩、人工智能的背后，都能看到香农公式的影子。但更重要的是，这套思维方式可以迁移到任何领域。

> **核心洞察**：信息的本质是"意外性"。你已经知道的事情，信息量为零；完全无法预测的随机噪音，信息量也为零。真正有价值的信息，是那些"降低不确定性"的内容——它既不是常识，也不是胡言乱语，而是恰好位于两者之间。

## 冗余：看起来浪费，实则必要

如果你问一个工程师："怎样才能让信息传递得又快又准？"他可能会说："去掉所有冗余。"但香农告诉我们，这个答案是错的。**没有冗余的信息系统极其脆弱**，任何一点干扰都会导致信息完全失真。

吴军用了一个生动的例子：英文中"I love you"有9个字符，但你即使只看到"I l_v_ y__"，也能猜出原文。这就是自然语言的冗余度在起作用。英语的冗余度大约在50%左右，这意味着你丢掉一半的字母，仍然能理解大部分意思。这种"浪费"恰恰是语言稳定传播的保障。

在工程领域，这个道理被称为"纠错码"。当你通过嘈杂的信道传递信息时，必须主动增加冗余——比如发送"000"来表示"0"，发送"111"来表示"1"。即使中途有一个比特翻转，接收端仍然能通过"多数投票"还原出原始信息。现代通信系统的奇迹，不在于它们消除了噪音，而在于它们学会了与噪音共存。

> **延伸思考**：这个原理同样适用于组织管理。为什么大公司需要层层汇报？为什么重要决策要开会讨论而不是单方面拍板？本质上都是在引入"冗余"来对抗信息传递中的失真。过度追求效率的扁平化组织，往往在复杂环境中反而更脆弱。

## 互信息：不是你说了什么，而是对方收到了什么

吴军在书中用了一个场景来解释"互信息"这个概念。假设你给朋友发微信说："明天见。"你自认为表达得很清楚，但对方可能理解成"明天早上8点图书馆见"，也可能理解成"明天晚上你家见"。**真正被传递的信息量，不是你发出了多少字，而是对方接收到的不确定性减少了多少**。

这就是"互信息"的核心：发送端和接收端共同理解的那部分信息，才是真正有效的信息。如果你说的话对方完全无法解码，那么无论你说多少，互信息都是零。这听起来是常识，但在实际沟通中，大部分人都在犯"自说自话"的错误。

想想技术文档为什么难读？因为作者脑子里有完整的上下文，他觉得"显而易见"的概念，读者可能闻所未闻。想想为什么会议总是低效？因为与会者各自带着不同的背景知识进场，同一句话在不同人耳朵里解码出了完全不同的意思。

**有效沟通的关键，是建立"共同的编码本"**。这就是为什么优秀的老师会从学生已知的东西讲起，为什么好的演讲总是充满类比和具体案例。你不是在展示自己知道多少，而是在帮助对方的脑子里构建起能够解码你信息的框架。

> **商业案例**：苹果公司的产品发布会堪称"互信息最大化"的典范。乔布斯从不堆砌参数，而是用"把1000首歌装进口袋"这样的具体场景，让普通人瞬间理解产品的价值。技术指标是发送端的信息，用户体验才是接收端的信息，苹果始终在优化后者。

## 信道容量：为什么不是所有管道都能承载高速信息

如果你用过拨号上网，一定记得那个刺耳的握手音。那是调制解调器在"试探"电话线的信道容量。电话线本来是为传输语音设计的，它的频率范围只有300Hz到3400Hz。你想用它传输数据，就只能在这个狭窄的"管道"里编码。这就是为什么拨号上网速度最高只能到56Kbps——不是调制解调器不够好，而是物理信道的容量就这么大。

香农用一个公式精确描述了这个限制：**C = B log₂(1 + S/N)**。其中C是信道容量，B是带宽，S/N是信噪比。这个公式告诉你三件事：

1. **带宽越宽，容量越大**——这就是为什么从4G到5G，核心是扩展频段。
2. **信噪比越高，容量越大**——这就是为什么光纤比电缆快，因为光信号几乎不受电磁干扰。
3. **容量不是无限的**——即使你把信噪比提升到天文数字，容量的增长也会逐渐放缓，最终触及极限。

这个公式的应用远不止通信工程。吴军用它来解释企业增长的瓶颈：一个公司的"信道容量"取决于它的组织带宽（能同时推进多少项目）和决策信噪比（在海量信息中识别关键信号的能力）。当一家公司声称"我们要做一切"时，它其实是在违背香农公式——没有无限容量的信道，试图什么都做的结果就是什么都做不好。

> **边界条件**：香农公式成立的前提是"平稳的加性高斯白噪声信道"。在现实中，噪声往往不是均匀分布的——比如突发的强干扰，或者恶意的攻击。这时候，简单套用公式会低估风险。这也是为什么工程实践中，系统设计都会留出充足的余量。

## 压缩与解压：如何用最少的比特说最多的话

ZIP文件、JPG图片、MP3音乐——你每天都在使用压缩技术，但你可能不知道它的理论基础来自香农。信息压缩的核心思想是：**去掉冗余，保留本质**。但什么是"冗余"，什么是"本质"，这需要精确的数学定义。

香农证明了：任何信息源都有一个"熵"，它是该信息源的理论压缩极限。比如英文文本的熵大约是每字符1比特（考虑到字母频率和语法规则），这意味着理论上，你可以把1GB的英文小说压缩到125MB左右。这就是为什么你能用ZIP把文档压缩到原来的1/8，但无论怎么努力，也不太可能压缩到1/20——熵是不可逾越的。

吴军区分了两种压缩：

**无损压缩**：保证解压后和原文件一模一样。适用于代码、文档等不能容忍任何错误的场景。它的压缩比受限于信息熵。

**有损压缩**：允许丢弃一些人类感知不到的细节。比如JPG会丢弃人眼不敏感的高频颜色变化，MP3会丢弃人耳听不到的超高频声音。这种压缩能达到惊人的比例（10:1甚至100:1），但代价是不可逆。

这里有一个深刻的洞察：**压缩的本质是"理解"**。你越理解数据的结构，就越能高效地压缩它。这就是为什么针对特定领域的压缩算法（比如医学影像压缩）总是比通用算法更强——它们利用了领域知识。机器学习的进步，某种程度上也是在提升机器"理解"数据的能力，从而实现更智能的压缩和表示。

> **跨领域类比**：学习也是一种压缩。当你刚接触一个领域时，每个概念都要单独记忆（低压缩比）。随着理解加深，你开始看到概念之间的联系，能用少数几个底层原则推导出大量结论（高压缩比）。这就是为什么大师讲起来"举重若轻"，而初学者总是"知识点堆砌"——前者已经完成了高度的信息压缩。

## 编码：给混沌的世界建立秩序

莫尔斯电码、ASCII、UTF-8、二维码——这些看起来千差万别的东西，本质上都在做同一件事：**把信息映射成可以传输和存储的符号序列**。编码的艺术，就是在"表达能力"和"传输效率"之间找到平衡。

吴军用了一个精彩的例子：为什么英文字母E对应的莫尔斯电码是"·"（一个点），而Z对应的是"-- ··"（两长两短）？因为E在英文中出现频率最高，Z最低。摩尔斯设计这套编码时，就已经在应用"高频符号用短码，低频符号用长码"的原则——这正是后来香农理论中"熵编码"的雏形。

现代编码理论更进一步。哈夫曼编码、算术编码这些算法，能够根据符号的实际出现概率，动态生成最优的编码方案。它们追求的目标是：让平均码长尽可能接近信息熵。这就是为什么ZIP压缩中，常用的词会被编成很短的代码，生僻词则对应较长的代码。

但编码不仅仅是为了压缩，还为了纠错。吴军详细讲解了"汉明码"和"里德-所罗门码"——它们通过巧妙的数学结构，能够在信息中嵌入"校验位"，让接收端不仅能发现错误，还能自动修正。这就是为什么你的手机在信号很差的地下车库仍然能打电话，为什么CD光盘划了几道痕仍然能播放——纠错码在默默工作。

> **技术演进**：从2G到5G，核心变化之一就是纠错码的进化。5G使用的LDPC码和Polar码，已经非常接近香农极限——在同样的信噪比下，它们几乎榨干了信道的每一比特容量。这也意味着，未来通信速度的提升，不能再依赖"更好的编码"，而必须拓展带宽或提高信噪比。

## 反馈：为什么闭环系统更稳定

信息论中有一个容易被忽视但极其重要的概念：**反馈**。在香农的通信模型中，发送端只管发送，接收端只管接收，中间没有互动。但现实中，最可靠的通信系统都是"带反馈"的。

想想你和朋友打电话。如果对方突然沉默了，你会问："喂？听得到吗？"这就是在建立反馈回路。TCP协议之所以比UDP可靠，核心就在于"确认机制"——每发送一个数据包，接收端都会回复"已收到"，如果超时未收到确认，发送端会自动重传。这种反馈机制让互联网在极其不可靠的物理链路上，实现了几乎100%的传输成功率。

吴军把这个原理延伸到了学习和创新领域。为什么硅谷的创业公司迭代速度快？因为它们建立了密集的反馈回路——小步快跑，快速上线，根据用户反馈调整。相比之下，传统企业往往花费数月打磨产品再发布，等拿到反馈时，市场可能已经变了。

**快速反馈不仅能提高成功率，还能加速学习**。在信息论的框架下，学习就是一个"减少不确定性"的过程。你做了一个假设（发送信息），观察结果（接收反馈），更新认知（调整编码本）。反馈越快，这个循环转得越快，你的认知系统就进化得越快。

> **教育启示**：传统教育的问题在于：学生做题，一周后老师才批改返回（反馈延迟）；考试分数只有一个数字，看不出具体哪里错了（反馈模糊）。优秀的在线教育平台会在你答题的瞬间给出反馈，并详细解释错在哪里——这种即时、精确的反馈让学习效率大幅提升。

## 噪声：你无法消除它，但可以利用它

在大部分人的理解中，噪声是敌人，应该被消灭。但信息论告诉你一个反直觉的事实：**噪声是信息传递的固有属性，你能做的不是消除它，而是与它共舞**。

香农第二定理（信道编码定理）证明了一个惊人的结论：只要传输速率低于信道容量，总是存在某种编码方式，使得错误率可以任意小——即使信道本身充满噪声。这意味着，在一个嘈杂的环境中，你依然可以实现几乎完美的通信，代价是需要更复杂的编码和更长的传输时间。

吴军用深空通信举例。旅行者1号探测器距离地球超过200亿公里，它发出的信号到达地球时，功率只有10的负16次方瓦——比一块手表电池还弱得多。但人类仍然能接收到清晰的图像和数据。这不是因为没有噪声，而是因为工程师们使用了极强的纠错码，用时间换取了可靠性。

更有趣的是，在某些场景下，噪声甚至是有益的。这个现象叫"随机共振"——在非线性系统中，适量的噪声能够增强微弱信号的检测。比如人类的神经系统，恰恰利用了一定程度的神经噪声来提高感知灵敏度。完全无噪的系统有时反而不如"有噪但鲁棒"的系统。

> **实践陷阱**：在商业决策中，很多人试图"等待完美信息"再行动，这就像在等一个无噪声的信道——永远不会出现。聪明的做法是承认噪声的存在，用冗余和反馈机制来对抗它，而不是瘫痪式等待。

## 信息增益：为什么有些问题值得问，有些不值得

如果你在玩"20个问题猜人物"的游戏，你的第一个问题会问什么？"他是男的吗？"这是个好问题，因为它能把可能性空间砍掉一半。"他姓Zhang吗？"这是个糟糕的问题，因为即使答案是"是"，你仍然面对一个巨大的不确定性。

这就是**信息增益**的思维方式：一个好问题，是那些能最大化减少不确定性的问题。在机器学习中，决策树算法就是用信息增益来选择分裂节点——每次选择那个能最大程度区分样本的特征。在科学研究中，关键实验就是那些"一旦做出结果，就能排除一大批假说"的实验。

吴军用这个框架来分析个人成长：为什么有些经历对你的成长帮助巨大，有些经历却收效甚微？关键在于信息增益。如果你已经是一个熟练的Java程序员，再花一年学C++，信息增益很小——因为两者的编程范式高度相似。但如果你去学函数式编程或机器学习，信息增益就大得多——它们会打开你思维中全新的区域。

**追求信息增益最大化，意味着你应该主动走出舒适区，去探索那些与你现有知识"正交"的领域**。这不是为了成为杂家，而是为了建立更丰富的心智模型。当不同领域的知识在你脑中交叉时，创新往往就在交叉点诞生。

> **阅读策略**：这也解释了为什么读同类书籍的边际收益递减。你读完第一本关于时间管理的书，收获巨大；读第二本，仍有启发；读到第十本，基本都是重复。聪明的做法是：在一个主题上读到"信息增益开始平缓"时就停下，转向另一个主题。

## 最大熵原理：在无知面前保持谦卑

在没有充分信息的情况下，你应该如何做假设？很多人会凭直觉猜测，或者套用以往经验。但信息论给出了一个更严谨的答案：**最大熵原理**——在满足已知约束条件的前提下，选择熵最大的概率分布。

这听起来抽象，但道理很简单：熵是"不确定性"的度量。当你对一件事所知甚少时，最诚实的态度就是承认不确定性，而不是假装知道。最大熵原理告诉你：不要在数据没有告诉你的地方添加额外的假设。

吴军举了一个例子。假设你知道一个骰子的平均值是3.5（这是唯一的约束条件），你能推断出每个面的概率吗？最大熵原理会给出答案：六个面概率相等，都是1/6。为什么？因为"均匀分布"是在满足"平均值3.5"这个约束下，熵最大的分布——它代表了你对其他信息的完全无知。

这个原理在机器学习中有广泛应用。很多算法（比如最大熵分类器）本质上就是在说：我只用训练数据告诉我的信息，不做任何额外的臆测。这种"奥卡姆剃刀"式的谨慎，反而让模型更加鲁棒。

> **哲学意义**：最大熵原理背后的哲学是：在不确定性面前，保持谦卑。很多人在信息不足时仍然会给出"确定"的判断，这不是自信，而是过度拟合。承认"我不知道"，并在这个基础上做概率性推理，往往比虚假的确定性更可靠。

## 数据、信息、知识：一个递进的阶梯

吴军在书中明确区分了三个层次：数据、信息、知识。这个区分看似简单，但理解透彻能避免很多混乱。

**数据**是原始的符号流，没有经过任何加工。比如传感器记录的一串数字，或者你手机里的照片文件。数据本身不传递意义，它只是"有待解读"的材料。

**信息**是能够减少不确定性的数据。当你把温度传感器的读数和"今天穿什么衣服"这个问题联系起来，数据就变成了信息。信息的价值在于它的上下文——同样一个数据，在不同场景下，信息量可能完全不同。

**知识**是被内化、结构化的信息，它能够指导行动。当你不仅知道今天20度，还知道"20度应该穿一件外套"，并且形成了一套关于气温与着装的经验系统，这就是知识。知识是可以复用、可以迁移的。

吴军指出，现代社会的一个陷阱是"数据过载，知识匮乏"。我们每天被海量数据淹没，但其中真正能转化为信息的很少，能进一步沉淀为知识的更少。**高效学习者和普通人的区别，不在于接触了多少数据，而在于转化率有多高**。

> **实践建议**：读书时不要追求"读完"，而要追求"转化"。读完一章后问自己：这改变了我对什么问题的看法？我能用这个思路解决什么实际问题？如果答案是"没有"，那这一章对你而言只是数据，不是知识。

## 信息不对称：世界运作的隐秘引擎

为什么二手车市场上"柠檬"（次品）总是多过好车？为什么保险公司要设置免赔额？为什么面试总是不准？这些看似无关的现象，背后都是**信息不对称**在起作用。

在二手车交易中，卖家知道车况，买家不知道。如果买家只愿意为"平均车况"付钱，那么车况好的车主就不愿意卖（他觉得亏了），最终市场上只剩下次品。这就是诺贝尔经济学奖得主阿克洛夫提出的"柠檬市场"理论——而这个理论的基础，正是信息论中的"信息不对称"。

吴军用信息论的语言重新解读了市场机制：**市场的核心功能之一，是减少信息不对称**。价格信号本身就是一种高度压缩的信息——它把供需关系、生产成本、消费者偏好等复杂因素，编码成了一个数字。当价格机制失灵时（比如行政管制导致价格失真），信息传递就会出问题，资源配置也随之扭曲。

在组织管理中，信息不对称同样无处不在。下属知道实际执行中的困难，领导不知道；领导知道战略背景，下属不知道。这就是为什么扁平化管理不是万能药——它减少了层级，但也可能减少了信息过滤和编码的过程，导致决策者被原始数据淹没。

> **警惕信息幻觉**：互联网时代，我们容易产生"信息对称幻觉"——觉得所有信息都可以上网查到。但真正关键的信息往往是不公开的：公司的真实财务状况、项目的内部问题、人际关系的微妙变化。过度相信公开信息，可能让你陷入"看得见的盲区"。

## 元信息：关于信息的信息

当你收到一封邮件，除了正文内容，你还能看到发件人、发送时间、主题、附件大小等信息。这些"关于信息的信息"，叫做**元信息**（metadata）。不要小看它们——在很多场景下，元信息比信息本身更重要。

吴军举了一个例子：美国国家安全局（NSA）的监控项目并不直接监听所有电话内容，而是收集元数据——谁给谁打电话，什么时间，通话多长时间。分析这些元数据，就能构建出一个人的社交网络、行为模式、甚至政治倾向。内容是"说了什么"，元信息是"与谁说、何时说、怎么说"——后者往往暴露更多。

在数据管理中，元信息决定了数据的可用性。一个没有标注的数据集几乎毫无价值——你不知道它来自哪里，什么时候采集的，用什么标准测量的。这就是为什么大公司会投入巨大资源做"数据治理"——本质上就是在完善元信息系统。

在个人知识管理中，元信息同样关键。你做笔记时，记录下"这是从哪本书的哪一章看到的""当时我在思考什么问题"，这些上下文就是元信息。有了它们，你才能在未来快速回忆起知识，建立知识之间的联系。

> **实践技巧**：建立个人知识库时，不要只保存"内容"，还要保存"来源、时间、当时的思考背景"。三个月后你再看这条笔记，没有元信息，你可能完全想不起当时为什么记这个；有了元信息，你能迅速重建当时的思维现场。

## 信息的时效性：新鲜度即价值

在股票市场，一条消息提前一秒知道，就可能意味着百万美元的差价。在战争中，情报的时效性直接决定胜负。在日常生活中，你提前一天知道某个政策变动，就能比别人多一天准备时间。**信息的价值，往往随时间指数级衰减**。

吴军用香农公式的一个推论来解释这个现象：信息量等于"意外性"。当一个消息是"新闻"时，它的意外性高，信息量大；当它变成"旧闻"时，所有人都知道了，意外性为零，信息量也为零。这就是为什么媒体行业永远在追逐"第一时间"——晚了一小时，即使内容完全相同，价值也可能腰斩。

但这里有一个悖论：**时效性强的信息，往往深度不足；深度足够的知识，往往时效性弱**。新闻报道要在事件发生后几小时内发出，没有时间深度调查；学术论文追求深度，但从投稿到发表往往需要一年以上。两者服务的是不同的需求。

对个人而言，这意味着你需要区分"快信息"和"慢知识"。前者让你不落伍，后者让你有深度。优秀的学习者会在两者之间建立平衡：用20%的时间跟踪行业动态（快信息），用80%的时间深耕基础原理（慢知识）。因为快信息的半衰期是几天，慢知识的半衰期可能是几十年。

> **投资启示**：巴菲特的投资哲学中有一条：不追逐热点，专注长期价值。用信息论的语言说，就是：他不参与"时效性信息"的竞争（因为那需要比所有人都快，他做不到），而是挖掘"被市场忽视的长期结构性信息"（比如公司的护城河、管理层的品格）。在信息时效性上你拼不过算法交易，但在信息深度上你可以超越大部分人。

## 冗余与创新：看似矛盾的共存

前面提到，冗余是对抗噪声的必要手段。但冗余不仅仅是防御性的，它还是创新的温床。这听起来矛盾——冗余不是浪费吗？怎么会促进创新？

吴军用生物进化来类比。DNA中有大量"垃圾基因"（当时认为没有功能的片段），这看起来是冗余。但正是这些冗余给了进化试错的空间——核心基因突变往往致命，但冗余基因突变后果较小，偶尔反而能产生有利性状。没有冗余，生物系统就太脆弱，无法在变化环境中生存。

在企业中，冗余体现为"多余的产能""闲置的人力""探索性的项目"。短期看这是成本，但长期看这是创新的种子。谷歌的"20%自由时间"政策，本质上就是制度化的冗余——它允许工程师把五分之一的时间花在"与本职工作无关"的项目上。Gmail、Google News这些后来的明星产品，都诞生于这个"冗余时间"。

**过度优化会扼杀创新**。当一个系统被压榨到极致效率，每一个零件都在满负荷运转，它就失去了试错的空间。一旦环境变化，它来不及调整。这就是为什么一味追求降本增效的公司，往往在范式转换时被颠覆——它们的系统太"紧"了，没有余量去适应新范式。

> **个人时间管理**：很多效率专家建议把日程排满，"最大化时间利用率"。但这是危险的。你需要在日程中留出"冗余时间"——没有明确目标的空闲，用来思考、闲逛、随机阅读。这些看似浪费的时间，往往是创造力萌发的时刻。乔布斯年轻时在印度游荡的经历、达尔文在船上发呆的时光，都是"冗余时间"的馈赠。

## 这本书的边界

《信息传》是一本优秀的科普书，但它也有明确的边界。首先，它主要讨论的是"通信意义上的信息"，对于"语义信息"（信息的意义和价值）涉及较少。香农本人曾明确说过，信息论不关心"信息的意义"，只关心"信息的传递"。所以当你试图用这套理论分析文学、艺术、哲学时，会发现它的解释力不足。

其次，书中的很多类比和延伸（比如用信息论分析商业、教育、个人成长）是吴军的个人洞察，并非严格的学术结论。这些类比启发性很强，但不要当作"科学真理"来套用。信息论是一个强大的思维工具，但它不是万能的。

第三，书中对数学推导做了大量简化，以便普通读者理解。如果你想真正掌握信息论，还需要阅读教科书和论文。这本书给你的是"地图"，不是"路径"——它告诉你哪里有风景，但具体怎么走，需要你自己探索。

> **使用场景限制**：信息论在处理"确定性系统的不确定性"时非常强大，但在面对"根本的未知未知"（你连可能性空间都不清楚的情况）时，它的作用有限。比如科学革命、范式转换、黑天鹅事件——这些往往不是"减少现有不确定性"，而是"发现了全新的不确定性"。

## 读完之后：三个可以立刻实践的思维习惯

**1. 主动寻求反馈**
无论是工作、学习还是创作，不要闭门造车。尽早展示半成品，收集反馈，快速迭代。记住：互信息最大化的关键是建立共同的编码本，而这需要通过反馈来校准。

**2. 区分信号与噪声**
当你被海量信息淹没时，问自己：这条信息真的减少了我的不确定性吗？还是只是重复的噪声？培养识别"高信息增益内容"的敏感度，减少低价值信息的摄入。

**3. 在确定性和冗余之间保持张力**
不要追求100%的效率，给自己的时间、注意力、资源留出余量。这些"浪费"是你应对不确定性的缓冲，也是未来创新的种子。

## 延伸阅读

如果这本书让你对信息论产生兴趣，可以继续读：

- 《信息简史》 - 詹姆斯·格雷克：从历史角度讲述信息概念的演变，文笔优美，视野宏大。
- 《数学之美》 - 吴军：同一作者的另一本书，从信息论、概率论角度解析自然语言处理，更偏技术应用。
- 《熵：一种新的世界观》 - 杰里米·里夫金：虽然有争议，但提供了从热力学熵到信息熵的跨学科思考。

如果你想真正掌握信息论，教科书推荐：
- Elements of Information Theory - Thomas M. Cover & Joy A. Thomas（信息论的经典教材，需要概率论基础）
