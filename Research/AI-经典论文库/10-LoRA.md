---
title: "论文精读：LoRA - Low-Rank Adaptation of Large Language Models"
tags: [AI, 论文, LoRA, 微调, LLM, PEFT]
date: 2026-02-28
paper_year: 2021
authors: Hu et al. (Microsoft)
arxiv: "https://arxiv.org/abs/2106.09685"
importance: "⭐⭐⭐⭐⭐ — 让普通人也能微调大模型"
---

# LoRA：让普通人也能微调大模型

> **一句话总结**：LoRA 通过在预训练模型的权重矩阵旁边插入两个小矩阵（低秩分解），只训练这些小矩阵就能达到接近全量微调的效果，参数量减少 99% 以上。

## 1. 这篇论文要解决什么问题？

2021 年，GPT-3 已经展示了大语言模型的强大能力。但如果你想让 GPT-3 更擅长某个特定任务（比如法律文本分析），你需要**微调（Fine-tuning）**——在你的数据上继续训练模型。

问题来了：

```
GPT-3 的参数量：175,000,000,000（1750亿）

全量微调意味着：
  - 存储：需要保存 1750亿个参数的副本
           ≈ 350 GB（FP16）
  - 显存：训练时需要存储梯度和优化器状态
           ≈ 1.2 TB 显存
  - 成本：需要数十张 A100 GPU
           ≈ 每次微调花费数万美元

而且每个任务需要一份完整的模型副本：
  法律 GPT-3：350 GB
  医疗 GPT-3：350 GB
  编程 GPT-3：350 GB
  ...

  存储成本爆炸！
```

**LoRA 的目标**：能不能只修改极少量参数，就达到全量微调的效果？

## 2. 核心直觉：压缩照片的启示

### 低秩分解的直觉

**比喻**：想象一张 1000x1000 的高清照片。

```
原始照片：1000 × 1000 = 1,000,000 个像素

但实际上，照片中有大量冗余！
（天空是一大片蓝色，草地是一大片绿色...）

SVD 分解：1000×1000 ≈ 1000×r × r×1000
                      （r 可以很小，比如 10）

参数量：1000×10 + 10×1000 = 20,000
       原来的 2%！而照片几乎看不出区别！
```

LoRA 的核心假设也是一样的：**微调时的权重变化矩阵是低秩的**——看似巨大的变化，其实只需要很少的"维度"就能描述。

### 为什么权重变化是低秩的？

```
直觉理解：

预训练模型已经学到了丰富的通用知识（高秩，复杂）
微调只是在这个基础上做"微调整"（低秩，简单）

比喻：
  预训练 = 一个什么都会的通才（知识结构复杂）
  微调   = 让这个通才多学一点法律知识（变化方向很少）

  你不需要重塑他的整个知识体系，
  只需要在几个"方向"上做调整就够了
```

## 3. LoRA 的核心公式

```
全量微调：
  W' = W + ΔW

  W：预训练权重（d×d 矩阵，比如 4096×4096）
  ΔW：微调带来的变化（同样 d×d）
  参数量：4096 × 4096 = 16,777,216

LoRA 的关键：用两个小矩阵近似 ΔW

  ΔW ≈ B × A

  B：d×r 矩阵（4096×8）
  A：r×d 矩阵（8×4096）
  参数量：4096×8 + 8×4096 = 65,536

  减少了 256 倍！

┌──────────────────────────────────────────┐
│                                          │
│   W' = W + B × A                         │
│         ↑    ↑   ↑                       │
│      冻结   可训练 可训练                 │
│    (不动)  (d×r)  (r×d)                  │
│                                          │
│   r 通常取 4, 8, 16, 32, 64             │
│                                          │
└──────────────────────────────────────────┘
```

### 可视化

```
传统全量微调：

  输入 x ──→ [W + ΔW] ──→ 输出 h
              16M 参数
              全部可训练

LoRA 微调：

  输入 x ──→ [W (冻结)] ──────────→ (+) ──→ 输出 h
              │                      ↑
              └──→ [A] ──→ [B] ──→──┘
                   r×d     d×r
                   可训练   可训练
                   ~65K 参数

前向传播：h = Wx + BAx
              ↑     ↑
           预训练  LoRA 增量
```

## 4. 初始化策略

```
A 的初始化：随机高斯（Kaiming 初始化）
B 的初始化：全零矩阵

为什么 B 初始化为零？
  → 训练开始时 BA = 0 × A = 0
  → ΔW = 0
  → 模型行为与预训练模型完全一致
  → 从一个好的起点开始训练！

这比随机初始化 ΔW 好得多——
你不会一上来就破坏预训练模型学到的知识。
```

## 5. 秩 r 的选择

秩 r 是 LoRA 最重要的超参数。它控制了"有多少个方向"可以调整。

```
┌──────┬───────────────┬──────────────────┬────────────┐
│  r   │  参数量(d=4096)│     效果          │  适用场景  │
├──────┼───────────────┼──────────────────┼────────────┤
│  1   │   8,192       │ 有限             │ 极端压缩   │
│  4   │   32,768      │ 不错             │ 简单任务   │
│  8   │   65,536      │ 很好             │ 多数场景   │
│ 16   │   131,072     │ 接近全量微调     │ 复杂任务   │
│ 64   │   524,288     │ ≈ 全量微调       │ 很少需要   │
│ 4096 │  16,777,216   │ = 全量微调       │ 不推荐     │
└──────┴───────────────┴──────────────────┴────────────┘

实践经验：
  - 大多数场景 r=8 或 r=16 就够了
  - 任务越复杂/越不同于预训练数据 → r 需要越大
  - 原论文发现：即使 r=1，某些任务也能取得不错效果！
```

## 6. 参数量对比：LoRA 到底省了多少？

```
以 LLaMA-7B 为例（7B 参数）：

┌─────────────────┬──────────────┬─────────────┐
│    微调方法      │  可训练参数   │  占比        │
├─────────────────┼──────────────┼─────────────┤
│ 全量微调        │ 7,000M       │ 100%        │
│ LoRA (r=8)      │ ~4M          │ 0.06%       │
│ LoRA (r=16)     │ ~8M          │ 0.11%       │
│ LoRA (r=64)     │ ~33M         │ 0.47%       │
└─────────────────┴──────────────┴─────────────┘

显存对比（7B 模型）：
  全量微调：~60 GB（需要 A100 80GB）
  LoRA：    ~18 GB（一张 RTX 4090 就够了！）
  QLoRA：   ~10 GB（一张 RTX 3090 就够了！）
```

## 7. LoRA 应用在哪些层？

```
Transformer 中的权重矩阵：

┌──────────────────────────────────────┐
│          Transformer 层              │
│                                      │
│  Self-Attention:                     │
│    W_q (Query)    ← 通常加 LoRA ✓   │
│    W_k (Key)      ← 可选            │
│    W_v (Value)    ← 通常加 LoRA ✓   │
│    W_o (Output)   ← 可选            │
│                                      │
│  Feed-Forward:                       │
│    W_up           ← 可选            │
│    W_down         ← 可选            │
│    W_gate         ← 可选            │
└──────────────────────────────────────┘

原论文发现：只对 W_q 和 W_v 加 LoRA 就效果很好
后续实践：全部加 LoRA 通常更好（参数量仍然很少）
```

## 8. QLoRA：4-bit 量化 + LoRA

QLoRA (2023) 是 LoRA 的重要进化，让微调大模型的门槛进一步降低。

```
QLoRA 的三大创新：

1. 4-bit NormalFloat 量化
   预训练权重 W：FP16 (16 bit) → NF4 (4 bit)
   显存直接减少 4 倍！

2. 双重量化 (Double Quantization)
   连量化参数本身也量化，进一步节省显存

3. 分页优化器 (Paged Optimizer)
   梯度存不下时自动换到 CPU 内存

效果：
┌─────────────────┬──────────────┬──────────────┐
│    方法          │  65B 模型    │   GPU 需求    │
├─────────────────┼──────────────┼──────────────┤
│ 全量微调        │ ~780 GB      │ 10× A100     │
│ LoRA (FP16)     │ ~160 GB      │ 2× A100      │
│ QLoRA (NF4)     │ ~48 GB       │ 1× A100      │
│ QLoRA (NF4) 7B  │ ~10 GB       │ 1× RTX 3090  │
└─────────────────┴──────────────┴──────────────┘
```

## 9. 实际应用场景

```
LoRA 的主要应用场景：

1. 领域适配
   通用 LLM → + 法律 LoRA → 法律助手
              + 医疗 LoRA → 医疗助手
              + 代码 LoRA → 编程助手

2. 风格迁移
   基础模型 → + 日系 LoRA → 日系风格图片 (Stable Diffusion)
             + 水墨 LoRA → 水墨风格图片

3. 多任务服务（LoRA 的独特优势）

   ┌─────────────────────────────────────────┐
   │         基础模型 (只需一份，共享)        │
   │                                         │
   │   ┌────────┐ ┌────────┐ ┌────────┐     │
   │   │法律LoRA│ │医疗LoRA│ │代码LoRA│     │
   │   │(~16MB) │ │(~16MB) │ │(~16MB) │     │
   │   └────────┘ └────────┘ └────────┘     │
   │                                         │
   │   根据请求动态加载不同的 LoRA 适配器     │
   │   切换成本几乎为零！                     │
   └─────────────────────────────────────────┘

4. 个人化模型
   每个用户一个 LoRA（几 MB），共享同一个基础模型
```

## 10. 代码示例

使用 Hugging Face PEFT 库，LoRA 微调只需几行代码：

```python
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM

# 加载预训练模型
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")

# 配置 LoRA
lora_config = LoraConfig(
    r=8,                      # 秩
    lora_alpha=16,            # 缩放因子
    target_modules=["q_proj", "v_proj"],  # 应用 LoRA 的层
    lora_dropout=0.05,        # Dropout
    bias="none",              # 不训练偏置
)

# 创建 LoRA 模型
model = get_peft_model(model, lora_config)

# 查看可训练参数量
model.print_trainable_parameters()
# 输出：trainable params: 4,194,304 || all params: 6,742,609,920
#       || trainable%: 0.0622%

# 之后正常训练即可，只有 LoRA 参数会被更新
```

## 11. LoRA 的后续发展

```
LoRA (2021)
  │
  ├──→ QLoRA (2023): 4-bit 量化 + LoRA，显存再降 4 倍
  │
  ├──→ DoRA (2024): 分解权重为方向和大小，效果更好
  │
  ├──→ LoRA+ (2024): A 和 B 用不同学习率，收敛更快
  │
  ├──→ rsLoRA (2024): 更好的秩缩放策略
  │
  ├──→ GaLore (2024): 全量微调的低秩投影，兼顾效果和效率
  │
  └──→ 各种 Merged LoRA 技术：合并多个 LoRA，组合不同能力
```

## 12. 关键收获

| 贡献 | 说明 |
|------|------|
| 参数高效微调 | 只需 0.06% 的参数就能达到全量微调的效果 |
| 降低门槛 | 让个人开发者也能微调大模型 |
| 即插即用 | 不改变模型架构，推理时可以合并回原始权重，零额外延迟 |
| 多任务灵活性 | 一个基础模型 + 多个 LoRA = 多个专业模型 |
| 理论洞察 | 揭示了微调过程的低秩本质 |

LoRA 真正实现了大模型的**民主化**——你不再需要巨额算力才能定制一个属于自己的 AI 模型。一张消费级显卡、几个小时的训练，就能得到一个针对你特定需求的专业模型。这正是 AI 开源社区蓬勃发展的关键基石之一。

---
## 相关笔记
- [[Research/AI-从零开始完整学习指南.md|AI 从零开始完整学习指南]]
- [[Research/AI-经典论文库/00-论文索引.md|经典论文索引]]
