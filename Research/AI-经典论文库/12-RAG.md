---
title: "论文精读：RAG - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
tags: [AI, 论文, RAG, 检索增强, LLM, 知识库]
date: 2026-02-28
paper_year: 2020
authors: Lewis et al. (Meta/Facebook AI)
arxiv: "https://arxiv.org/abs/2005.11401"
importance: "⭐⭐⭐⭐⭐ — 解决 LLM 幻觉的核心方案"
---

# RAG：给大模型装上"外挂知识库"

## 一句话总结

RAG 让语言模型在回答问题时先去"查资料"再回答，而不是纯靠记忆——就像从"闭卷考试"升级到"开卷考试"。

## 核心问题：大模型的两个致命缺陷

大语言模型（LLM）再强大，也逃不开两个根本问题：

```
缺陷 1：知识冻结
┌──────────────────────────────────────┐
│ GPT-3 的训练数据截止到 2021 年 6 月    │
│ → 问它 2024 年谁赢了奥运会？不知道！   │
│ → 它的知识是"冷冻"的，不会自动更新     │
└──────────────────────────────────────┘

缺陷 2：幻觉 (Hallucination)
┌──────────────────────────────────────┐
│ 问：张三丰是哪本小说的角色？           │
│ LLM：张三丰是《射雕英雄传》的主角...   │
│ → 一本正经地胡说八道！                 │
│ → 它不知道自己不知道                   │
└──────────────────────────────────────┘
```

**根本原因**：LLM 把所有知识都"压缩"在参数里。就像一个学生把整本百科全书都背下来——背不全很正常，张冠李戴也很正常。

## RAG 的直觉："开卷考试"

```
┌─────────────────────────────────────────────────────┐
│            闭卷考试 vs 开卷考试                       │
├─────────────────────────────────────────────────────┤
│                                                     │
│  闭卷考试（纯 LLM）：                                │
│  学生 ← 问题 → 纯靠记忆回答                         │
│  ✗ 记不全  ✗ 可能记错  ✗ 新知识不知道                │
│                                                     │
│  开卷考试（RAG）：                                    │
│  学生 ← 问题 → 先翻书找相关段落 → 基于原文回答       │
│  ✓ 信息准确  ✓ 可以引用来源  ✓ 知识可更新            │
│                                                     │
└─────────────────────────────────────────────────────┘
```

RAG 的核心逻辑极其简单：**先搜索，再回答**。

## RAG 架构详解

```
RAG 完整流程：

 用户问题："量子计算的最新进展是什么？"
      │
      ▼
 ┌──────────────┐
 │  问题编码器   │  把问题转成向量
 │   (DPR)      │
 └──────┬───────┘
        │ 查询向量 q
        ▼
 ┌──────────────┐     ┌─────────────────┐
 │  向量检索     │────►│  知识库          │
 │  (MIPS)      │     │  (维基百科等)     │
 └──────┬───────┘     │  2100万文档段落   │
        │              └─────────────────┘
        │ Top-K 相关段落
        ▼
 ┌──────────────────────────────────────┐
 │  拼接：[问题] + [检索到的段落1,2,...K] │
 └──────────────┬───────────────────────┘
                │
                ▼
 ┌──────────────┐
 │  生成器       │  基于问题+参考资料生成回答
 │  (BART/GPT)  │
 └──────┬───────┘
        │
        ▼
 "根据最新研究，量子计算在...方面取得了突破..."
```

### 两大核心组件

**1. 检索器 (Retriever) —— "图书管理员"**

传统搜索靠关键词匹配（TF-IDF、BM25），但这太死板了：
- 搜"总统官邸"找不到"白宫"
- 搜"心脏病治疗"找不到"冠状动脉搭桥手术"

RAG 使用 **DPR (Dense Passage Retrieval)**——用神经网络把文档和查询都变成向量，在向量空间中找"语义最近"的文档：

```
关键词搜索 vs 向量搜索：

关键词搜索：
  查询 "狗的品种" → 只匹配包含"狗""品种"的文档
  ✗ 找不到 "金毛猎犬是一种温顺的犬类"

向量搜索 (DPR)：
  查询 "狗的品种" → 编码为向量 → 找语义相近的向量
  ✓ 能找到 "金毛猎犬是一种温顺的犬类"（语义相关！）
```

**2. 生成器 (Generator) —— "作文高手"**

拿到检索结果后，生成器（论文中用的是 BART）把问题和参考资料"融合"起来，生成流畅的回答。

## RAG-Sequence vs RAG-Token

论文提出了两种变体，区别在于如何使用检索到的多个文档：

```
假设检索到 K=3 个文档：Doc1, Doc2, Doc3

RAG-Sequence（每个文档独立生成，最后投票）：
┌─────────────────────────────────┐
│ 问题 + Doc1 → 生成完整回答 A1   │
│ 问题 + Doc2 → 生成完整回答 A2   │
│ 问题 + Doc3 → 生成完整回答 A3   │
│ → 综合概率选最优                 │
└─────────────────────────────────┘

RAG-Token（每生成一个词都可以参考不同文档）：
┌─────────────────────────────────────────┐
│ 生成第 1 个词：参考 Doc1 概率最高 → "量子"│
│ 生成第 2 个词：参考 Doc3 概率最高 → "计算"│
│ 生成第 3 个词：参考 Doc2 概率最高 → "已经"│
│ → 逐词灵活切换参考来源                    │
└─────────────────────────────────────────┘
```

- **RAG-Sequence**：像写论文时一次只参考一篇文献，写完再换
- **RAG-Token**：像写论文时每句话都可能参考不同文献

实验发现两者性能接近，RAG-Token 在开放式生成任务上略优。

## 实验结果

```
在知识密集型 NLP 任务上的表现：

Natural Questions (开放域问答)：
  纯 BART:          26.5 EM
  RAG-Token:        44.5 EM  ← 提升 68%！

TriviaQA (trivia 问答)：
  纯 BART:          -
  RAG-Token:        56.8 EM

事实验证 (FEVER)：
  RAG 达到了接近 SOTA 的水平

关键发现：RAG 生成的回答更准确、更具体、更少幻觉
```

## 2024-2025 年 RAG 的演进

原始 RAG 论文发表后，这个方向发展迅猛：

```
RAG 演进路线：

Naive RAG (2020)
│  简单的"检索+生成"
│
├── Advanced RAG (2023-2024)
│   ├── 查询改写 (Query Rewriting)
│   ├── 混合检索 (Hybrid Search)：向量+关键词
│   ├── 重排序 (Reranking)：用交叉编码器精排
│   ├── 自适应分块 (Adaptive Chunking)
│   └── 上下文压缩 (Context Compression)
│
├── Modular RAG (2024)
│   ├── 可插拔的检索/生成模块
│   └── 多跳推理检索
│
└── Agentic RAG (2025)
    ├── AI Agent 自主决定"要不要检索"
    ├── 多轮检索 + 自我反思
    └── 工具调用 + RAG 融合
```

## 工程实践要点

如果你要在实际项目中用 RAG，需要关注：

```
实际 RAG 系统架构：

文档 → 分块(Chunking) → 向量化(Embedding) → 向量数据库
                                                  │
用户查询 → 向量化 → 检索 Top-K → 重排序 → 拼接 Prompt → LLM → 回答

关键工程决策：
┌────────────────┬──────────────────────────────────┐
│ 分块策略        │ 按段落？按语义？固定长度？重叠？   │
│ 向量模型        │ OpenAI Ada? BGE? Jina?            │
│ 向量数据库      │ Pinecone? Milvus? Chroma? Weaviate│
│ 检索数量 K      │ 太少信息不够，太多噪声太大         │
│ 重排序          │ Cohere Reranker? Cross-Encoder?   │
│ Prompt 工程     │ 如何有效利用检索到的上下文         │
└────────────────┴──────────────────────────────────┘
```

## 核心 takeaway

| 要点 | 内容 |
|------|------|
| 核心思想 | 先检索相关知识，再基于知识生成回答 |
| 解决的问题 | LLM 知识冻结和幻觉 |
| 关键技术 | DPR 向量检索 + BART/GPT 生成 |
| 为什么重要 | 几乎所有企业级 LLM 应用都在用 RAG |
| 实际影响 | ChatGPT 联网搜索、Perplexity、企业知识库问答 |

RAG 可能是论文列表中**对工业界影响最直接**的一篇——今天你用的几乎每一个"能联网""能查资料"的 AI 产品，背后都有 RAG 的影子。

---
## 相关笔记
- [[Research/AI-从零开始完整学习指南.md|AI 从零开始完整学习指南]]
- [[Research/AI-经典论文库/00-论文索引.md|经典论文索引]]
