---
title: "论文精读：Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
tags: [AI, 论文, CoT, 推理, 提示工程, LLM]
date: 2026-02-28
paper_year: 2022
authors: Wei et al. (Google Brain)
arxiv: "https://arxiv.org/abs/2201.11903"
importance: "⭐⭐⭐⭐ — 让大模型学会\"思考\""
---

# Chain-of-Thought：让大模型"展示解题过程"

## 一句话总结

只需要在提示中加入"思考步骤"的示例，就能让大语言模型的推理能力大幅提升——就像数学老师要求学生"写出解题过程"一样。

## 核心发现

这篇论文发现了一个简单到令人难以置信的现象：

```
标准提示 (Standard Prompting)：
Q: 食堂有 23 个苹果，用了 20 个做午餐，又买了 6 个。还有多少？
A: 9

→ 模型直接给答案，经常算错

Chain-of-Thought 提示：
Q: 食堂有 23 个苹果，用了 20 个做午餐，又买了 6 个。还有多少？
A: 食堂原来有 23 个苹果。用了 20 个后还剩 23-20=3 个。
   又买了 6 个，所以现在有 3+6=9 个。答案是 9。

→ 模型"一步步想"，准确率暴涨！
```

就这么简单？就这么简单。但效果惊人。

## 直觉："数学老师"比喻

```
┌─────────────────────────────────────────────────────┐
│              为什么"写过程"这么重要？                  │
├─────────────────────────────────────────────────────┤
│                                                     │
│  想想你上数学课的经历：                               │
│                                                     │
│  差学生：看完题目 → 直接猜答案 → 经常错              │
│  好学生：看完题目 → 列出已知条件 → 分步计算 → 检查   │
│                                                     │
│  老师的要求：                                        │
│  "不要只写答案！把过程写出来！"                       │
│                                                     │
│  原因：                                              │
│  1. 分步计算降低了每一步的难度                        │
│  2. 中间步骤提供了"脚手架"                           │
│  3. 如果某步错了，后面还有机会纠正                    │
│                                                     │
│  CoT 对大模型做的就是同样的事！                       │
└─────────────────────────────────────────────────────┘
```

## Few-shot CoT vs Zero-shot CoT

论文主要讨论的是 **Few-shot CoT**，后续工作又发现了更简单的 **Zero-shot CoT**：

```
方法 1：Few-shot CoT（论文原始方法）
──────────────────────────────────
在 prompt 里给几个带推理过程的示例：

Q: [示例问题1]
A: [思考步骤1]...[答案1]

Q: [示例问题2]
A: [思考步骤2]...[答案2]

Q: [实际问题]
A: （模型自动生成思考步骤+答案）


方法 2：Zero-shot CoT（后续工作，Kojima et al. 2022）
──────────────────────────────────────────────────
不需要示例，只加一句话：

Q: [问题]
A: Let's think step by step.

→ 就这一句话，模型就会自动展开推理！
```

Zero-shot CoT 的发现更加震撼——原来大模型"会"推理，只是没人"提醒"它要推理。就像一个聪明但懒的学生，你只需要说"想想再答"，他就能答得好得多。

## 涌现性质：大模型专属能力

CoT 有一个非常重要的特性——**它只在足够大的模型上有效**：

```
模型大小 vs CoT 效果：

参数量         标准提示    CoT 提示    CoT 增益
──────────────────────────────────────────────
422M (小)       ~10%       ~10%        无效果
8B (中)         ~15%       ~16%        几乎无效
62B (大)        ~20%       ~35%        +15%
175B (巨大)     ~17.7%     ~58.1%      +40.4% !!!
540B (PaLM)     ~17.7%     ~58.1%      巨大提升

    效果
    ↑
    │              ╱ CoT
    │             ╱
    │        ____╱
    │   ____╱
    │──╱─────────── 标准提示
    │╱
    └──────────────────→ 模型大小
         ↑
     ~100B 参数是"门槛"
```

这是一种**涌现能力 (Emergent Ability)**：小模型加了 CoT 反而可能变差（因为它生成的"推理步骤"是乱的），只有大到一定程度，CoT 才开始发挥作用。

## 关键实验结果

论文在多个推理基准上做了测试，结果令人印象深刻：

```
GSM8K（小学数学应用题，8500 题）：

PaLM 540B + 标准提示:        17.7%
PaLM 540B + CoT:             58.1%  ← 提升 3.3 倍！

对比：经过专门微调的模型:     55.0%
→ CoT 仅靠提示就超过了微调！

其他基准：
┌──────────────────────┬──────────┬──────────┐
│ 任务                  │ 标准提示  │ CoT      │
├──────────────────────┼──────────┼──────────┤
│ MultiArith (数学)     │ 33.8%    │ 78.7%    │
│ SVAMP (数学)          │ 69.2%    │ 79.0%    │
│ AQuA (代数)           │ 35.8%    │ 48.3%    │
│ 日期推理              │ 49.3%    │ 67.5%    │
│ 运动理解              │ 52.4%    │ 95.4%    │
└──────────────────────┴──────────┴──────────┘
```

## CoT 为什么有效？深层原因

```
直觉解释：

1. 分解复杂问题
   "A有5个苹果，给了B3个，B又给了C1个，C有几个？"

   不用 CoT：模型要"一步到位"算出答案 → 容易错
   用 CoT：模型分步算 → 每一步都很简单

2. 分配更多"计算资源"
   Transformer 每生成一个 token 做一次前向传播
   → 中间推理步骤 = 额外的计算步骤
   → 相当于给模型更多"思考时间"

3. 提供推理"脚手架"
   每一步的输出成为下一步的输入
   → 形成信息传递链
   → 模型可以追踪中间状态

本质：CoT 把一个难题拆成了一串简单题
```

## 后续发展：思维的进化

CoT 引发了一系列关于"让模型更好推理"的研究浪潮：

```
CoT 家族发展树：

Chain-of-Thought (2022) — 线性推理链
│
├── Self-Consistency (2023)
│   └── 生成多条推理链，投票选最优
│
├── Tree-of-Thought (2023)
│   └── 探索多条推理路径，像下棋一样搜索
│
│   思维树示意：
│        问题
│       / | \
│      /  |  \
│    想法1 想法2 想法3
│    / \    |    \
│   ↓   ↓   ↓     ↓
│   评估哪条路径最有希望 → 继续深入
│
├── Graph-of-Thought (2024)
│   └── 推理步骤可以合并、分叉、形成图结构
│
├── Meta-CoT (2024-2025)
│   └── 模型自己决定需不需要 CoT、用什么形式的 CoT
│
└── OpenAI o1/o3 (2024-2025)
    └── 将 CoT "内化"到模型训练中
    └── 模型自动产生长思维链（隐藏的 CoT）
    └── 在数学/编程上达到专家水平
```

特别值得一提的是 **OpenAI o1/o3 系列**——它们本质上就是把 CoT 从"提示技巧"升级为"内置能力"。模型在回答前会自动进行长时间的内部推理，然后才输出最终答案。

## CoT 的局限性

```
CoT 不是万能的：

1. 小模型无效 — 需要 >100B 参数
2. 简单任务多余 — 查询事实不需要推理
3. 可能产生错误推理 — "看起来合理但实际上是错的"
4. 增加 token 消耗 — 推理步骤 = 更多输出 = 更贵
5. 无法保证正确性 — CoT 不是形式化证明
```

## 核心 takeaway

| 要点 | 内容 |
|------|------|
| 核心发现 | 让模型展示推理步骤能大幅提升推理能力 |
| 方法 | Few-shot CoT（给示例）或 Zero-shot CoT（"Let's think step by step"） |
| 门槛 | 只在大模型（>100B 参数）上有效，涌现能力 |
| 效果 | GSM8K 准确率从 17.7% 飙升到 58.1% |
| 深远影响 | 催生了 o1/o3 等推理模型，改变了 prompt engineering |

CoT 可能是 prompt engineering 领域最重要的发现之一。它揭示了一个深刻的道理：**大模型的推理能力并非不存在，而是需要被正确地"唤醒"**。

---
## 相关笔记
- [[Research/AI-从零开始完整学习指南.md|AI 从零开始完整学习指南]]
- [[Research/AI-经典论文库/00-论文索引.md|经典论文索引]]
