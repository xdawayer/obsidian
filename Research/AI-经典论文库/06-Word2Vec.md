---
title: "论文精读：Efficient Estimation of Word Representations in Vector Space"
tags: [AI, 论文, Word2Vec, 词向量, NLP]
date: 2026-02-28
paper_year: 2013
authors: Mikolov et al. (Google)
arxiv: "https://arxiv.org/abs/1301.3781"
importance: "⭐⭐⭐⭐ — 让计算机理解词义的里程碑"
---

# Word2Vec：让计算机理解词义的里程碑

> **一句话总结**：Word2Vec 提出了两种高效的神经网络架构（CBOW 和 Skip-gram），能把每个词映射成一个稠密向量，使得语义相近的词在向量空间中距离也近。

## 1. 这篇论文要解决什么问题？

在 Word2Vec 之前，计算机处理文字的方式非常原始——**独热编码（One-Hot Encoding）**。假设词汇表有 10 万个词，那么每个词就是一个 10 万维的向量，只有对应位置是 1，其余全是 0。

这种表示方法有两个致命问题：

```
"猫" = [0, 0, 1, 0, 0, ..., 0]   (第3位为1)
"狗" = [0, 0, 0, 0, 1, ..., 0]   (第5位为1)
"汽车" = [0, 1, 0, 0, 0, ..., 0]  (第2位为1)

问题1：任意两个词的距离都相同！"猫"和"狗"的距离 = "猫"和"汽车"的距离
问题2：10万维向量，极度稀疏浪费
```

Mikolov 的目标很明确：**找到一种方法，让每个词变成一个低维（比如 300 维）的稠密向量，同时保留词之间的语义关系。**

## 2. 核心直觉：词的"地图坐标"

想象你画一张**语义地图**：

```
              [高兴]
               |
        [开心]---[快乐]
               |
              [愉悦]

  [悲伤]---[难过]---[忧愁]


        [苹果]---[香蕉]---[橙子]

                [汽车]---[卡车]---[公交车]
```

在这张地图上：
- **意思相近的词挨在一起**（"开心"靠近"快乐"）
- **不相关的词离得远**（"开心"远离"汽车"）
- **关系可以用向量运算表达**

Word2Vec 就是要自动学出这样一张"地图"——每个词的坐标就是它的**词向量（Word Embedding）**。

## 3. 两种模型架构

Word2Vec 提出了两种训练模型，核心思想都基于一个语言学假设：**一个词的含义由它周围的词决定**（distributional hypothesis）。

### 3.1 CBOW（连续词袋模型）

**思路**：用上下文预测中心词。

```
输入（上下文）          输出（目标词）

  "今天"  ──┐
  "天气"  ──┤
             ├──→ 预测 ──→ "好"
  "真"    ──┤
  "啊"    ──┘

句子："今天 天气 真 [好] 啊"
```

就像完形填空：看到周围的词，猜中间那个词是什么。

### 3.2 Skip-gram

**思路**：用中心词预测上下文。方向完全反过来。

```
输入（中心词）          输出（上下文）

                  ┌──→ "今天"
                  ├──→ "天气"
  "好"  ─────────┤
                  ├──→ "真"
                  └──→ "啊"

句子："今天 天气 真 [好] 啊"
```

就像造句：给你一个词"好"，猜它周围可能出现哪些词。

### 两者对比

```
┌──────────────┬──────────────────┬──────────────────┐
│     对比     │      CBOW        │    Skip-gram     │
├──────────────┼──────────────────┼──────────────────┤
│ 输入→输出    │ 上下文 → 中心词  │ 中心词 → 上下文  │
│ 训练速度     │ 更快             │ 较慢             │
│ 高频词效果   │ 更好             │ 一般             │
│ 低频词效果   │ 一般             │ 更好             │
│ 适用场景     │ 大规模语料       │ 小规模语料       │
└──────────────┴──────────────────┴──────────────────┘
```

实践中 **Skip-gram** 用得更多，因为它对罕见词的表示更好。

## 4. "King - Man + Woman = Queen"

这是 Word2Vec 最著名的发现——**词向量的线性关系**：

```
      vec("King") - vec("Man") + vec("Woman") ≈ vec("Queen")

    这意味着：
    "国王"和"男人"的关系 ≈ "女后"和"女人"的关系

    向量空间中的可视化：

         Man ─────────────→ Woman
          |                   |
          | (王权方向)         | (王权方向)
          ↓                   ↓
         King ────────────→ Queen
              (性别方向)
```

类似的关系比比皆是：
- `vec("巴黎") - vec("法国") + vec("日本") ≈ vec("东京")`（首都关系）
- `vec("走") - vec("走了") + vec("跑了") ≈ vec("跑")`（时态关系）

**这说明 Word2Vec 不仅学到了词的含义，还自动捕获了词之间的抽象关系！**

## 5. 负采样：让训练成为可能

原始的 Skip-gram 有个大问题：输出层是一个 softmax，要计算词汇表中每个词的概率。如果词汇表有 100 万个词，每次训练都要计算 100 万次——太慢了！

**负采样（Negative Sampling）** 是一个天才的训练技巧：

```
原始任务：在100万个词中找出正确答案（太难）

    "好" → softmax(100万个词) → P("天气") = ?

简化任务：区分"真实搭档"和"随机路人"（简单多了）

    ("好", "天气")  → 这是真实的上下文对吗？ → Yes ✓
    ("好", "恐龙")  → 这是真实的上下文对吗？ → No  ✗
    ("好", "键盘")  → 这是真实的上下文对吗？ → No  ✗
    ("好", "沙发")  → 这是真实的上下文对吗？ → No  ✗
```

**比喻**：原来是在全校 1000 人中找你的同桌（太难），现在变成——给你同桌和随机抽的 5 个人，问你谁是你同桌（太简单了）。

每次只采样 5-20 个"负样本"（随机词），计算量从 100 万降到了几十。这让在大规模语料上训练成为可能。

## 6. 训练过程可视化

```
语料库："我 喜欢 吃 苹果 和 香蕉"

窗口大小=2，使用 Skip-gram：

步骤1：中心词="喜欢"
  正样本对：(喜欢, 我), (喜欢, 吃)
  负样本对：(喜欢, 汽车), (喜欢, 月亮), ...

步骤2：中心词="吃"
  正样本对：(吃, 喜欢), (吃, 苹果)
  负样本对：(吃, 电脑), (吃, 星星), ...

        ↓ 反复迭代训练 ↓

结果：每个词获得一个 300 维向量
  vec("苹果") ≈ vec("香蕉")  ← 经常出现在相似上下文中
  vec("苹果") ≠ vec("汽车")  ← 上下文完全不同
```

## 7. 对后续 NLP 的深远影响

Word2Vec 的影响怎么强调都不为过：

### 直接影响
- **GloVe (2014)**：Stanford 提出的全局词向量，结合了矩阵分解和 Word2Vec 的优点
- **FastText (2016)**：Facebook 扩展 Word2Vec，考虑子词信息，解决 OOV（未登录词）问题

### 间接影响
- **ELMo (2018)**：从静态词向量走向上下文相关的词向量
- **BERT (2018)**：Transformer + 预训练，可以看作 Word2Vec 思想在更深层次的延续
- **GPT 系列**：大语言模型的 embedding 层本质上就是词向量的进化版

### 思想遗产

```
Word2Vec 的核心洞察：
"用预测任务自监督地学习表示"

           ↓ 这个思想贯穿了整个深度学习

图像领域：对比学习 (SimCLR, CLIP)
语音领域：Wav2Vec
图结构：Node2Vec, Graph2Vec
推荐系统：Item2Vec
```

**Word2Vec 证明了：不需要人工标注，仅靠大量无标注文本，就能学到有意义的语义表示。** 这个思想是后来所有预训练模型的基石。

## 8. 局限性

- **静态向量**：每个词只有一个向量，无法处理一词多义（"苹果"是水果还是公司？）
- **窗口限制**：只看局部上下文，无法捕获长距离依赖
- **词级别**：无法处理未见过的词（OOV 问题）

这些局限性后来分别被 ELMo、Transformer 和 BPE 分词解决。

## 9. 关键收获

| 贡献 | 说明 |
|------|------|
| 词向量概念普及 | 让 NLP 从离散空间进入连续空间 |
| 高效训练方法 | 负采样让大规模训练成为可能 |
| 语义线性关系 | 发现词向量的代数运算能表达语义 |
| 预训练思想 | 开创了"先预训练、再微调"的范式 |

---
## 相关笔记
- [[Research/AI-从零开始完整学习指南.md|AI 从零开始完整学习指南]]
- [[Research/AI-经典论文库/00-论文索引.md|经典论文索引]]
