---
title: "论文精读：Denoising Diffusion Probabilistic Models"
tags: [AI, 论文, Diffusion, 扩散模型, 图像生成, 生成模型]
date: 2026-02-28
paper_year: 2020
authors: Ho et al.
arxiv: "https://arxiv.org/abs/2006.11239"
importance: "⭐⭐⭐⭐⭐ — Stable Diffusion/DALL-E 的理论基础"
---

# DDPM：Stable Diffusion / DALL-E 的理论基础

> **一句话总结**：DDPM 证明了一个惊人的思路——先把图片逐步变成纯噪声，然后训练一个神经网络学会逆转这个过程，就能从纯噪声中生成逼真的图片。

## 1. 这篇论文要解决什么问题？

2020 年之前，图像生成领域的霸主是 **GAN（生成对抗网络）**。GAN 能生成漂亮的图片，但有几个让人头疼的问题：

```
GAN 的问题：

1. 训练不稳定
   生成器 vs 判别器 → 像两个人博弈 → 经常一方碾压另一方 → 训练崩溃

2. 模式崩塌 (Mode Collapse)
   生成器偷懒 → 只生成几种"安全"的图片 → 多样性差

3. 难以评估
   没有明确的似然函数 → 不知道模型到底学得好不好
```

DDPM 提供了一个全新的思路：**不需要对抗训练，通过"加噪-去噪"的过程就能生成高质量图片**。

## 2. 核心直觉：墨水与清水

**比喻**：想象你把一滴墨水滴进一杯清水中。

```
前向过程（加噪声）：

  🖼️ 清晰图片 → 略有噪点 → 更模糊 → ... → 纯噪声（白色杂点）

  时间 t=0                                      时间 t=T

  ┌──────┐   ┌──────┐   ┌──────┐       ┌──────┐
  │ 🐱   │ → │ 🐱~  │ → │ ~~   │ → ... │ ████ │
  │ 猫咪 │   │ 有噪 │   │ 很模 │       │ 纯噪 │
  │ 清晰 │   │ 点了 │   │ 糊了 │       │ 声了 │
  └──────┘   └──────┘   └──────┘       └──────┘

墨水滴入清水：
  清水(图片) → 开始扩散 → 逐渐浑浊 → ... → 完全均匀（纯噪声）
```

**关键洞察**：加噪声很容易（有数学公式），但**如果我们能学会逆转这个过程**——从纯噪声中一步步恢复出清晰图片——那我们就有了一个生成模型！

```
反向过程（去噪声）= 生成过程：

  ┌──────┐       ┌──────┐   ┌──────┐   ┌──────┐
  │ ████ │ → ... │ ~~   │ → │ 🐱~  │ → │ 🐱   │
  │ 纯噪 │       │ 隐约 │   │ 快清 │   │ 清晰 │
  │ 声   │       │ 有形 │   │ 晰了 │   │ 猫咪 │
  └──────┘       └──────┘   └──────┘   └──────┘

  从随机噪声开始 → 每一步去掉一点噪声 → 最终得到清晰图片！
```

## 3. 前向过程：逐步加噪声

前向过程是一个**马尔科夫链**——每一步只依赖上一步，与更早的步骤无关。

```
数学描述：

  q(x_t | x_{t-1}) = N(x_t; √(1-β_t) × x_{t-1}, β_t × I)

翻译成人话：
  第 t 步的图片 = √(1-β_t) × 上一步的图片 + √β_t × 随机噪声

其中 β_t 是噪声调度表（noise schedule），控制每一步加多少噪声：
  β₁ = 0.0001（开始时加很少噪声）
  β₂ = 0.0002
  ...
  β_T = 0.02（最后加较多噪声）

  总步数 T = 1000（原论文设定）
```

一个优雅的数学性质：**不需要真的跑 1000 步，可以一步到位**！

```
直接从 x₀ 跳到任意 x_t：

  x_t = √ᾱ_t × x₀ + √(1-ᾱ_t) × ε

其中 ᾱ_t = α₁ × α₂ × ... × α_t，αᵢ = 1-βᵢ
     ε ~ N(0, I)（标准高斯噪声）

这个性质让训练变得非常高效！
```

## 4. 反向过程：神经网络学习去噪

反向过程是 DDPM 的核心——训练一个神经网络 ε_θ 来预测每一步加的噪声。

```
训练过程（极其简单优雅）：

重复以下步骤直到收敛：
  1. 从数据集中取一张图片 x₀
  2. 随机选一个时间步 t ~ Uniform(1, T)
  3. 随机采样噪声 ε ~ N(0, I)
  4. 加噪：x_t = √ᾱ_t × x₀ + √(1-ᾱ_t) × ε
  5. 让神经网络预测噪声：ε_θ(x_t, t)
  6. 损失函数：L = ||ε - ε_θ(x_t, t)||²
                        ↑           ↑
                    真实噪声    网络预测的噪声

就这么简单！训练目标就是：给你一张加了噪声的图和时间步 t，
让你猜我加了什么噪声。
```

### 训练过程可视化

```
┌─────────────────────────────────────────────────────┐
│                                                     │
│  x₀ (原图)                                          │
│    ↓                                                │
│  随机选 t=500, 采样噪声 ε                            │
│    ↓                                                │
│  x₅₀₀ = √ᾱ₅₀₀ × x₀ + √(1-ᾱ₅₀₀) × ε              │
│    ↓                                                │
│  ┌──────────────────────┐                           │
│  │   U-Net 神经网络      │                           │
│  │   输入：x₅₀₀ 和 t=500 │ → ε_θ (预测的噪声)       │
│  └──────────────────────┘                           │
│    ↓                                                │
│  Loss = ||ε - ε_θ||²   → 反向传播 → 更新网络参数    │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 生成过程（推理）

```
从纯噪声开始，一步步去噪：

  x_T ~ N(0, I)        ← 随机采样纯噪声

  for t = T, T-1, ..., 1:
    预测噪声：ε_θ(x_t, t)
    去噪一步：x_{t-1} = 去噪公式(x_t, ε_θ, t)

  return x₀              ← 最终的清晰图片！

去噪公式（简化版）：
  x_{t-1} = (1/√α_t) × (x_t - (β_t/√(1-ᾱ_t)) × ε_θ(x_t, t)) + σ_t × z
                                                                      ↑
                                                              少量随机噪声
                                                             （增加多样性）
```

## 5. U-Net：去噪网络的架构

DDPM 使用的神经网络是 **U-Net**，这是一个编码器-解码器架构，带有跳跃连接：

```
U-Net 架构示意：

输入 x_t                                      输出 ε_θ
  ↓                                              ↑
┌────┐                                      ┌────┐
│64×64│─────────── 跳跃连接 ──────────────→ │64×64│
└─┬──┘                                      └─┬──┘
  ↓ 下采样                               上采样 ↑
┌────┐                                      ┌────┐
│32×32│─────────── 跳跃连接 ──────────────→ │32×32│
└─┬──┘                                      └─┬──┘
  ↓                                          ↑
┌────┐                                      ┌────┐
│16×16│─────────── 跳跃连接 ──────────────→ │16×16│
└─┬──┘                                      └─┬──┘
  ↓                                          ↑
  └────────────→ ┌────┐ ───────────────────┘
                 │8×8  │  (瓶颈层)
                 └────┘

时间步 t 通过正弦位置编码注入到每一层中
```

## 6. 与 GAN 的对比

```
┌────────────────┬──────────────────┬──────────────────┐
│                │       GAN        │      DDPM        │
├────────────────┼──────────────────┼──────────────────┤
│ 训练方式       │ 对抗训练         │ 最大似然/去噪    │
│                │ (博弈，不稳定)   │ (简单MSE损失)    │
├────────────────┼──────────────────┼──────────────────┤
│ 训练稳定性     │ 差（经常崩溃）   │ 非常稳定         │
├────────────────┼──────────────────┼──────────────────┤
│ 模式崩塌       │ 常见             │ 几乎没有         │
├────────────────┼──────────────────┼──────────────────┤
│ 生成质量       │ 高               │ 更高             │
├────────────────┼──────────────────┼──────────────────┤
│ 生成多样性     │ 有限             │ 高               │
├────────────────┼──────────────────┼──────────────────┤
│ 生成速度       │ 极快（一步）     │ 慢（需要~1000步）│
├────────────────┼──────────────────┼──────────────────┤
│ 似然估计       │ 无               │ 有               │
└────────────────┴──────────────────┴──────────────────┘

DDPM 用生成速度换取了：更稳定的训练 + 更高的质量 + 更好的多样性
```

## 7. 为什么 DDPM 如此重要？

DDPM 本身生成的图片质量还不是最顶级的，但它**开创了一个全新的范式**，后续的改进让扩散模型成为了图像生成的绝对霸主：

```
DDPM (2020)
  │
  ├──→ Improved DDPM (2021): 更好的噪声调度，更少的步数
  │
  ├──→ DDIM (2020): 确定性采样，大幅加速（1000步→50步）
  │
  ├──→ Classifier-Free Guidance (2022): 条件生成的关键技巧
  │
  ├──→ Latent Diffusion / Stable Diffusion (2022):
  │     在潜空间做扩散 → 速度大幅提升 → 消费级GPU可用
  │
  ├──→ DALL-E 2 (2022, OpenAI): 文本→图像
  │
  ├──→ Midjourney (2022): 商业化文生图
  │
  ├──→ Stable Diffusion 3 (2024): 更高质量
  │
  └──→ 视频生成：Sora (2024), Runway Gen-3...
```

### Stable Diffusion 的核心改进

```
DDPM 的问题：在像素空间做扩散太慢了（256×256×3 维）

Stable Diffusion 的解决方案：

  ┌─────────┐     ┌──────────┐     ┌──────────┐
  │ 256×256 │ →   │ 编码器   │  →  │ 32×32    │
  │ 像素空间│     │ (VAE)    │     │ 潜空间   │
  └─────────┘     └──────────┘     └────┬─────┘
                                        ↓
                                  在潜空间做扩散
                                  (维度降低64倍!)
                                        ↓
  ┌─────────┐     ┌──────────┐     ┌────┴─────┐
  │ 256×256 │ ←   │ 解码器   │  ←  │ 32×32    │
  │ 像素空间│     │ (VAE)    │     │ 潜空间   │
  └─────────┘     └──────────┘     └──────────┘
```

## 8. 数学优雅性

DDPM 的美在于它的数学非常干净。整个训练过程可以被推导为**变分下界（ELBO）** 的优化，而最终简化为一个简单的 MSE 损失。

```
理论推导路径：

最大化数据似然 log p(x₀)
    ↓
变分下界 (ELBO)
    ↓
分解为 T 个 KL 散度项
    ↓
每个 KL 散度 = 预测噪声的 MSE 损失
    ↓
最终训练目标：L = E[||ε - ε_θ(x_t, t)||²]

从复杂的概率论出发 → 最终得到一个无比简洁的损失函数
这就是数学之美！
```

## 9. 关键收获

| 贡献 | 说明 |
|------|------|
| 全新生成范式 | "加噪-去噪"替代对抗训练 |
| 训练极其稳定 | 简单的 MSE 损失，不需要对抗博弈 |
| 高质量多样性 | 超越 GAN 的生成质量和多样性 |
| 催生产业革命 | Stable Diffusion, DALL-E, Midjourney 的理论基础 |

DDPM 的故事告诉我们：**一个看似"绕远路"的方法（先加噪再去噪），可能比直接的方法（GAN 的一步生成）更优雅、更强大。** 有时候，慢即是快。

---
## 相关笔记
- [[Research/AI-从零开始完整学习指南.md|AI 从零开始完整学习指南]]
- [[Research/AI-经典论文库/00-论文索引.md|经典论文索引]]
