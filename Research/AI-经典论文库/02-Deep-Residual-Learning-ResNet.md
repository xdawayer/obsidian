---
title: "论文精读：Deep Residual Learning for Image Recognition"
tags: [AI, 论文, ResNet, CNN, 计算机视觉, 深度学习]
date: 2026-02-28
paper_year: 2015
authors: He et al. (Microsoft Research)
arxiv: "https://arxiv.org/abs/1512.03385"
importance: "⭐⭐⭐⭐⭐ — 解决了深度网络训练的核心难题"
---

# 论文精读：Deep Residual Learning for Image Recognition (2015)

## 一句话总结

**通过简单而天才的"残差连接"（跳跃连接），让神经网络从几十层成功扩展到上百层甚至上千层，彻底突破了深度学习的深度瓶颈。**

## 为什么重要？

打个比方：你在建一座摩天大楼。理论上楼层越高视野越好，但现实中建到 20 层就开始摇晃，建到 30 层直接塌了——不是材料不好，而是结构设计有问题。

ResNet 的贡献就像发明了一种全新的建筑结构：在每几层之间加一根"支撑钢缆"（残差连接），让大楼可以稳稳地建到 152 层，甚至理论上可以建到 1000 层以上。

这篇论文直接让何恺明成为计算机视觉领域的传奇人物，也让 ResNet 成为几乎所有视觉模型的基础组件。时至今日，残差连接已经渗透到 AI 的每一个角落——包括 Transformer 里也大量使用。

## 背景与动机

### 深度网络的"怪现象"

2015 年之前，大家有一个朴素的直觉：**网络越深，效果应该越好**。毕竟更多层意味着更强的表达能力，对吧？

但实验中出现了一个违反直觉的怪现象：

```
退化问题 (Degradation Problem)：

训练误差 ↑
  │
  │    ╭── 56层网络（反而更差！）
  │   ╱
  │  ╱   ╭── 20层网络
  │ ╱   ╱
  │╱   ╱
  ├───╱────────────────→ 训练轮次
  │
  注意：这不是过拟合！连训练误差都变差了！
```

**关键洞察：** 这不是过拟合（过拟合是训练好但测试差），而是训练本身就失败了。56 层网络的训练误差比 20 层还高——更深的网络反而更难优化。

### 为什么会退化？

理论上，一个 56 层网络至少应该和 20 层一样好——最差情况下，多出来的 36 层学成"什么都不做"（恒等映射）就行了。但实际上，让网络学习"什么都不做"比想象中困难得多，因为优化器很难找到这个解。

## 核心创新

### 1. 残差连接 (Skip Connection / Shortcut Connection)

> **比喻：抄近道**

想象你要从 A 点走到 B 点，正常路线要翻过一座大山（层层计算）。残差连接就是在山脚开了一条隧道——信息可以"抄近道"直达 B 点。

```
普通网络 vs 残差网络：

  普通网络：                   残差网络：
  ┌─────────┐                 ┌─────────┐
  │  输入 x  │                 │  输入 x  │─────────────┐
  └────┬─────┘                 └────┬─────┘             │
       ↓                            ↓                   │ 抄近道！
  ┌─────────┐                 ┌─────────┐              │ (Identity
  │  卷积层  │                 │  卷积层  │              │  Shortcut)
  └────┬─────┘                 └────┬─────┘             │
       ↓                            ↓                   │
  ┌─────────┐                 ┌─────────┐              │
  │  卷积层  │                 │  卷积层  │              │
  └────┬─────┘                 └────┬─────┘             │
       ↓                            ↓                   │
  ┌─────────┐                 ┌──────────────┐         │
  │ 输出 H(x)│                 │ 输出 F(x) + x │◄────────┘
  └─────────┘                 └──────────────┘

  目标：学习 H(x)               目标：学习 F(x) = H(x) - x
  （直接学完整映射）              （只需学"差异/残差"）
```

**为什么学残差更容易？**

- 普通网络要学：$H(x)$（完整映射）
- 残差网络只需学：$F(x) = H(x) - x$（输入和输出的差异）

如果最优解接近恒等映射（即这几层不需要做太多变化），那 $F(x)$ 接近 0，学习"趋近于零"远比学习"复杂的恒等映射"容易。就像考试时，直接抄答案（跳跃连接传过来的 x）再做小修改，比从头作答容易多了。

### 2. 恒等映射 (Identity Mapping)

残差连接最优雅的地方在于：**不需要额外参数，不需要额外计算**。

```
x ──────────────────────────────┐
│                               │
↓                               │
[权重层] → ReLU → [权重层]      │
│                               │
↓                               │
+ ◄─────────────────────────────┘  (逐元素相加)
│
↓
ReLU
│
↓
输出
```

当输入和输出维度相同时，直接相加（恒等快捷连接）。维度不同时，用 1x1 卷积做投影匹配。

### 3. Bottleneck 结构

对于更深的网络（50 层以上），直接堆叠 3x3 卷积太耗计算。Bottleneck 结构通过"先压缩再展开"节省计算量：

```
Bottleneck 残差块（用于 ResNet-50/101/152）：

  输入 (256维)
      │──────────────────────┐
      ↓                      │
  [1×1 卷积, 64维]  ← 压缩！ │
      ↓                      │
  [3×3 卷积, 64维]  ← 处理   │
      ↓                      │
  [1×1 卷积, 256维] ← 展开！ │
      ↓                      │
      + ◄────────────────────┘
      ↓
    输出 (256维)

  计算量对比：
  普通：两个 3×3×256 = 1,179,648 参数
  瓶颈：1×1×64 + 3×3×64 + 1×1×256 = 69,632 参数
  节省了 94% 的计算！
```

### 4. 从 18 层到 152 层：实验对比

```
各版本 ResNet 架构一览：

  ResNet-18:   [2, 2, 2, 2]  基础块    11M 参数
  ResNet-34:   [3, 4, 6, 3]  基础块    21M 参数
  ResNet-50:   [3, 4, 6, 3]  瓶颈块    25M 参数
  ResNet-101:  [3, 4, 23, 3] 瓶颈块    44M 参数
  ResNet-152:  [3, 8, 36, 3] 瓶颈块    60M 参数

  ImageNet Top-5 错误率趋势：
  ━━━━━━━━━━━━━━━━━━━━━━━━━━
  VGG-16 (2014):        7.32%
  ResNet-34:             5.71%  ← 比 VGG 更深且更好
  ResNet-50:             5.25%
  ResNet-101:            4.60%
  ResNet-152:            4.49%  ← 人类水平约 5.1%！
  ResNet 集成:           3.57%  ← 超越人类视觉！
```

## 关键公式解读

残差学习的核心公式极其简洁：

$$y = F(x, \{W_i\}) + x$$

| 部分 | 含义 | 直觉 |
|------|------|------|
| $x$ | 残差块的输入 | "底稿" — 你已有的答案 |
| $F(x, \{W_i\})$ | 残差函数（要学习的部分） | "修改意见" — 在底稿上做的改动 |
| $y$ | 残差块的输出 | "终稿" = 底稿 + 修改意见 |

梯度传播的视角更能解释为什么 ResNet 有效：

$$\frac{\partial \text{Loss}}{\partial x} = \frac{\partial \text{Loss}}{\partial y} \cdot \left(1 + \frac{\partial F}{\partial x}\right)$$

注意那个 **"1 +"**！即使 $\frac{\partial F}{\partial x}$ 很小甚至为 0，梯度仍然可以通过"1"这条路直达底层，不会消失。这就是为什么 152 层的网络也能顺利训练。

## 实验结果

### ImageNet 图像分类

| 模型 | 层数 | Top-1 错误率 | Top-5 错误率 |
|------|------|-------------|-------------|
| VGG-19 | 19 | 27.3% | 8.6% |
| Plain-34（无残差） | 34 | 28.5% | 10.0% |
| **ResNet-34** | 34 | **24.5%** | **7.8%** |
| **ResNet-152** | 152 | **21.4%** | **5.7%** |

关键对比：
- Plain-34 比 Plain-18 **更差**（退化现象）
- ResNet-34 比 ResNet-18 **更好**（残差连接解决了退化）
- ResNet-152 错误率 **低于人类水平**（约 5.1%）

### 其他任务

ResNet 还赢得了 2015 年 ILSVRC 和 COCO 竞赛的 **5 个赛道冠军**：
- ImageNet 分类
- ImageNet 检测
- ImageNet 定位
- COCO 检测
- COCO 分割

## 后续影响

```
ResNet (2015)
    │
    ├── 直接改进
    │   ├── ResNeXt (2017) — 分组卷积 + 残差
    │   ├── DenseNet (2017) — 密集连接（每层连接所有前面的层）
    │   ├── SE-Net (2018) — 加入通道注意力
    │   └── EfficientNet (2019) — 系统化缩放
    │
    ├── 思想延伸
    │   ├── Transformer 中的 Add & Norm — 本质就是残差连接！
    │   ├── U-Net 的跳跃连接 — 医学图像分割的基石
    │   └── Highway Networks — 带门控的残差（略早于 ResNet）
    │
    └── 工程影响
        ├── 几乎所有深度网络都标配残差连接
        ├── 让"越深越好"重新成为可能
        └── 启发了对网络深度/宽度的系统研究
```

残差连接的思想如此基础和通用，以至于今天很难找到一个**不用**残差连接的深度学习模型。

## 推荐阅读顺序

读这篇论文前，建议先了解：

1. **卷积神经网络基础** — 卷积、池化、特征图
2. **AlexNet (2012)** — 深度学习在视觉领域的突破
3. **VGGNet (2014)** — 理解"更深的普通网络"的极限
4. **BatchNorm (2015)** — ResNet 大量使用的归一化技术

读完这篇之后，建议继续阅读：
- DenseNet — 残差连接的进化版
- [[Research/AI-经典论文库/01-Attention-Is-All-You-Need|Transformer]] — 残差连接在 NLP 中的应用

## 参考资源

- **李沐精读系列**：[ResNet 论文逐段精读](https://www.bilibili.com/video/BV1P3411y7nn/) — 最推荐的中文讲解
- **何恺明本人演讲**：[Deep Residual Learning](https://www.youtube.com/watch?v=C6tLw-rPQ2o) — 第一手理解作者思路
- **CS231n (Stanford)**：卷积神经网络课程 — 系统学习 CNN 架构演进
- **动手学深度学习**：[ResNet 章节](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html) — 带代码实现

---
## 相关笔记
- [[Research/AI-从零开始完整学习指南.md|AI 从零开始完整学习指南]]
- [[Research/AI-经典论文库/00-论文索引.md|经典论文索引]]
