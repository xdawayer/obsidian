---
title: "论文精读：Adam - A Method for Stochastic Optimization"
tags: [AI, 论文, Adam, 优化器, 深度学习]
date: 2026-02-28
paper_year: 2014
authors: Kingma & Ba
arxiv: "https://arxiv.org/abs/1412.6980"
importance: "⭐⭐⭐⭐ — 深度学习最常用的优化器"
---

# Adam：深度学习最常用的优化器

> **一句话总结**：Adam 结合了 Momentum（动量）和 RMSProp（自适应学习率）两种优化技巧，让深度学习训练既快又稳，成为事实上的默认优化器。

## 1. 这篇论文要解决什么问题？

训练神经网络的本质是**优化**——在一个极其复杂的地形中找到最低点。这个"地形"就是损失函数的曲面。

```
损失函数地形示意（2D 截面）：

损失值
  ^
  |   *                          *
  |    \    *                  /
  |     \  / \      局部最优  /
  |      \/   \       ↓     /
  |            \     /\    /
  |             \   /  \  /
  |              \ /    \/  ← 全局最优
  |               *
  +──────────────────────────→ 参数值
```

**问题是：怎么在这个地形中高效地"下山"？**

最简单的方法是 SGD（随机梯度下降），但它有很多问题。Adam 就是为了解决这些问题而生的。

## 2. 从 SGD 到 Adam：下山策略的演进

### 2.1 朴素 SGD：盲人摸石头下山

```
策略：摸一下脚下的坡度，往最陡的方向走一步

参数更新：θ = θ - lr × ∇L

问题：
  ┌─ 学习率太大 → 在谷底来回震荡，无法收敛
  ├─ 学习率太小 → 下山速度极慢
  ├─ 所有参数用同一个学习率 → 不同参数的最佳步幅不同
  └─ 梯度方向噪声大 → 走的路径很曲折
```

**比喻**：就像一个盲人在山上，每一步只能摸到脚下很小一块区域的坡度。他走的路径摇摇晃晃，既慢又不稳。

### 2.2 Momentum（动量）：给下山者一个"惯性"

```
策略：不仅看当前坡度，还考虑之前走的方向（积累惯性）

v_t = β₁ × v_(t-1) + (1-β₁) × ∇L    ← 速度 = 惯性 + 当前梯度
θ = θ - lr × v_t

可视化：
  没有动量（SGD）         有动量（Momentum）

    ╱╲╱╲╱╲               ─────────→
   ╱      ╲╱╲               ↓
              ╲╱╲         ─────→
                 ╲╱          ↓
                             →
   (来回震荡，缓慢下降)    (平滑快速地冲向最低点)
```

**比喻**：给下山的盲人一个滚动的铁球。铁球有惯性，不会被小石头（噪声）干扰方向，能更平滑地滚向谷底。而且在平坦处因为惯性还能继续前进，不容易卡在局部最优。

### 2.3 RMSProp：每个参数有自己的步幅

```
策略：根据历史梯度的大小，自动调整每个参数的学习率

s_t = β₂ × s_(t-1) + (1-β₂) × (∇L)²   ← 累积梯度的平方
θ = θ - lr / √(s_t + ε) × ∇L

直觉：
  梯度一直很大的参数 → s_t 大 → 学习率自动变小 → 走小步
  梯度一直很小的参数 → s_t 小 → 学习率自动变大 → 走大步
```

**比喻**：想象山上有些方向很陡（梯度大），有些方向很平缓（梯度小）。聪明的下山者会在陡峭方向走小步（避免冲过头），在平缓方向走大步（加速前进）。

### 2.4 Adam = Momentum + RMSProp

Adam 把两个技巧结合起来：**既有惯性，又有自适应步幅**。

```
Adam 的完整算法：

初始化：m₀ = 0, v₀ = 0, t = 0

每一步：
  t = t + 1

  ① 计算梯度
     g_t = ∇L(θ_(t-1))

  ② 更新一阶矩估计（动量）
     m_t = β₁ × m_(t-1) + (1-β₁) × g_t

  ③ 更新二阶矩估计（自适应学习率）
     v_t = β₂ × v_(t-1) + (1-β₂) × g_t²

  ④ 偏差校正（关键步骤！）
     m̂_t = m_t / (1 - β₁ᵗ)
     v̂_t = v_t / (1 - β₂ᵗ)

  ⑤ 更新参数
     θ_t = θ_(t-1) - lr × m̂_t / (√v̂_t + ε)

默认超参数：lr=0.001, β₁=0.9, β₂=0.999, ε=1e-8
```

## 3. 偏差校正：容易被忽略的关键

为什么需要偏差校正？因为 m 和 v 初始化为 0，**训练初期的估计值偏小**。

```
举例（β₁ = 0.9）：

步骤1：m₁ = 0.9×0 + 0.1×g₁ = 0.1×g₁        ← 只有真实值的10%！
步骤2：m₂ = 0.9×0.1g₁ + 0.1×g₂ = 0.19×均值   ← 还是偏小

偏差校正后：
步骤1：m̂₁ = 0.1×g₁ / (1-0.9¹) = g₁          ← 恢复正常！
步骤2：m̂₂ = 0.19×均值 / (1-0.9²) = 均值       ← 正确！

随着 t 增大，(1 - β₁ᵗ) → 1，校正的影响逐渐消失
```

**比喻**：就像一个新来的天气预报员，前几天的预测主要靠猜（因为历史数据太少），偏差校正就是根据"你其实只积累了几天数据"这个事实来调整预测。

## 4. Adam 的直觉图解

```
SGD:          像盲人摸石头下山
               → 慢、不稳、容易迷路

Momentum:     像给盲人一个铁球
               → 有惯性，更平滑，但步幅固定

RMSProp:      像给盲人一双智能鞋（自动调节步幅）
               → 陡的地方走小步，平的地方走大步

Adam:         铁球 + 智能鞋
               → 既有惯性又有自适应步幅
               → 快速、稳定、几乎不需要调参

┌──────────────────────────────────────────────┐
│                                              │
│   Adam = 一阶矩(方向) + 二阶矩(步幅) +     │
│          偏差校正(初期准确性)                 │
│                                              │
└──────────────────────────────────────────────┘
```

## 5. 什么时候用什么优化器？

```
┌─────────────────┬─────────────────────────────────────┐
│    优化器       │         适用场景                     │
├─────────────────┼─────────────────────────────────────┤
│ SGD + Momentum  │ 计算机视觉（ResNet 等）             │
│                 │ 追求最终精度，不怕训练慢             │
│                 │ 有充足时间调学习率                   │
├─────────────────┼─────────────────────────────────────┤
│ Adam            │ NLP、生成模型、强化学习              │
│                 │ 快速原型验证                         │
│                 │ 不想花时间调参                       │
├─────────────────┼─────────────────────────────────────┤
│ AdamW           │ 大模型预训练（GPT、BERT）            │
│                 │ 需要正则化的场景                     │
│                 │ 当前最佳实践的默认选择               │
├─────────────────┼─────────────────────────────────────┤
│ LAMB / LARS     │ 超大 batch size 分布式训练           │
└─────────────────┴─────────────────────────────────────┘
```

### Adam vs SGD 的经典之争

```
训练损失曲线对比：

损失
 ^
 |  SGD
 | ╲
 |  ╲          Adam
 |   ╲        ╱ (快速下降)
 |    ╲      ╱
 |     ╲    ╱
 |      ╲──╱───── Adam 先收敛
 |       ╲╱
 |        ╲────── 但 SGD 最终可能找到更好的解
 |         ╲
 +───────────────→ 训练步数
```

一个经典发现：**Adam 收敛更快，但 SGD（调好学习率后）往往能找到泛化更好的解**。这被称为 Adam 的"泛化鸿沟"（generalization gap）。

### AdamW：修复 Adam 的 weight decay 问题

Adam 原论文中的 L2 正则化实现有误——它把 weight decay 和自适应学习率耦合在一起了。

```
Adam + L2（有问题）：
  梯度中加入 weight decay → 被自适应学习率缩放 → 正则化效果不一致

AdamW（正确做法）：
  先用 Adam 更新 → 再单独做 weight decay → 正则化效果一致

AdamW: θ_t = θ_(t-1) - lr × (m̂_t/√v̂_t + λ × θ_(t-1))
                                              ↑ 解耦的 weight decay
```

如今，**AdamW 已经取代 Adam 成为大模型训练的标准选择**。

## 6. 超参数实用指南

```
┌──────────┬──────────────┬──────────────────────────┐
│  超参数  │   默认值     │         建议              │
├──────────┼──────────────┼──────────────────────────┤
│ lr       │ 0.001        │ 通常 1e-4 到 3e-4        │
│          │              │ 大模型用 1e-4 或更小      │
├──────────┼──────────────┼──────────────────────────┤
│ β₁       │ 0.9          │ 几乎不需要改             │
├──────────┼──────────────┼──────────────────────────┤
│ β₂       │ 0.999        │ 大模型有时用 0.95        │
├──────────┼──────────────┼──────────────────────────┤
│ ε        │ 1e-8         │ 混合精度训练用 1e-5      │
├──────────┼──────────────┼──────────────────────────┤
│ wd       │ 0            │ AdamW 用 0.01 或 0.1     │
└──────────┴──────────────┴──────────────────────────┘
```

## 7. 关键收获

| 贡献 | 说明 |
|------|------|
| 结合两大优化技巧 | Momentum（方向）+ RMSProp（步幅） |
| 偏差校正 | 解决训练初期估计偏差问题 |
| 近乎免调参 | 默认超参数在大多数场景下都能工作 |
| 工业标准 | 成为深度学习事实上的默认优化器 |

Adam 的论文至今被引用超过 **18 万次**，是深度学习领域被引用最多的论文之一。它的成功不仅在于理论优雅，更在于**实用性极强**——几乎任何深度学习项目的第一次尝试，用 Adam 都不会太差。

---
## 相关笔记
- [[Research/AI-从零开始完整学习指南.md|AI 从零开始完整学习指南]]
- [[Research/AI-经典论文库/00-论文索引.md|经典论文索引]]
