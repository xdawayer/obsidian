---
title: "论文精读：GPT-3 - Language Models are Few-Shot Learners"
tags: [AI, 论文, GPT, LLM, 大语言模型, Few-shot]
date: 2026-02-28
paper_year: 2020
authors: Brown et al. (OpenAI)
arxiv: "https://arxiv.org/abs/2005.14165"
importance: "⭐⭐⭐⭐⭐ — 大语言模型时代的开山之作"
---

# 论文精读：GPT-3 - Language Models are Few-Shot Learners (2020)

## 一句话总结

**通过把语言模型暴力扩大到 1750 亿参数，GPT-3 展示了一种惊人的能力——不需要微调，只给几个例子甚至只给一句指令，就能完成各种从未见过的任务，开启了大语言模型时代。**

## 为什么重要？

打个比方：BERT 时代的 AI 像是一个"应试教育生"——每换一门考试都要专门备考（微调），虽然有通用知识底子，但离开了针对性训练就不行。

GPT-3 则像一个"天才通才"——你只需要跟他说"来，给你看个例子，照着做"（few-shot），甚至只给一句话的指示"帮我翻译这句话"（zero-shot），他就能做得有模有样。

更重要的是，GPT-3 揭示了一个深刻的规律：**当模型大到一定程度，量变会引起质变**。这个发现直接催生了后来的 ChatGPT、GPT-4、Claude 等一系列改变世界的产品。

## 背景与动机

### 之前的方法有什么问题？

2020 年之前，NLP 的标准范式是 BERT 确立的"预训练 + 微调"：

```
BERT 时代的问题：

1. 每个任务都需要微调
   ━━━━━━━━━━━━━━━━━━━━
   情感分析 → 收集标注数据 → 微调 → 部署
   问答系统 → 收集标注数据 → 微调 → 部署
   文本摘要 → 收集标注数据 → 微调 → 部署
   ......
   每做一个新任务就要重来一遍！

2. 需要大量标注数据
   ━━━━━━━━━━━━━━━━
   标注数据贵、慢、有偏差
   很多小众任务根本没有足够数据

3. 分布外泛化差
   ━━━━━━━━━━━━━━━━
   在训练集上表现好 ≠ 现实世界表现好
   微调可能导致模型过度适配特定数据集
```

**GPT-3 的野心：** 能不能训练一个足够大的模型，让它什么都会，而不需要针对每个任务单独微调？

### 从 GPT-1 到 GPT-3 的进化

```
模型规模的指数级增长：

GPT-1 (2018):    1.17 亿参数   ████
GPT-2 (2019):    15 亿参数     ████████████
GPT-3 (2020):    1750 亿参数   ████████████████████████████████████████

                               GPT-3 = 117 个 GPT-2
                               GPT-3 = 1496 个 GPT-1
```

## 核心创新

### 1. 规模定律 (Scaling Laws)：大力出奇迹

GPT-3 最震撼的发现是：**模型越大，能力越强——不仅是渐进提升，而是跨越式提升**。

```
GPT-3 的训练资源：

参数量:    175,000,000,000 (1750亿)
训练数据:  ~570GB 文本（约 3000 亿 token）
          ├── Common Crawl (过滤后)  60%
          ├── WebText2               22%
          ├── Books1 & Books2        16%
          └── Wikipedia               3%
训练成本:  约 460 万美元（估算）
GPU 时间:  约 3640 PetaFLOP/s-days

直觉理解 1750亿参数有多大：
━━━━━━━━━━━━━━━━━━━━━━━━━━━
  如果每个参数是一粒沙子
  1750亿粒沙子 ≈ 填满一间小教室

  如果每秒数一个参数
  需要数 5548 年才能数完
```

论文测试了 8 个不同规模的模型（从 1.25 亿到 1750 亿参数），发现几乎所有任务上都呈现出平滑的 **对数线性提升**：

```
模型规模 vs 任务表现：

准确率 ↑
  │           ╭── GPT-3 175B
  │         ╱
  │       ╱
  │     ╱  ╭── GPT-3 13B
  │   ╱  ╱
  │  ╱ ╱
  │╱╱
  ├─── ╭── GPT-3 1.3B
  │  ╱
  │╱
  ├── GPT-3 125M
  └────────────────────────→ 参数量 (log scale)
```

### 2. Zero-shot / One-shot / Few-shot Learning

GPT-3 定义了三种无需微调的使用方式：

```
三种学习方式对比：
━━━━━━━━━━━━━━━━━

(1) Zero-shot（零样本）：只给任务描述
┌──────────────────────────────────┐
│ 请将以下英文翻译成中文：          │ ← 任务描述
│ "The weather is nice today."     │ ← 输入
│ →                                │
└──────────────────────────────────┘
模型输出："今天天气很好。"

(2) One-shot（单样本）：给一个例子
┌──────────────────────────────────┐
│ 请将英文翻译成中文。              │ ← 任务描述
│ "Hello" → "你好"                 │ ← 一个示例
│ "The weather is nice today." →   │ ← 输入
└──────────────────────────────────┘
模型输出："今天天气很好。"

(3) Few-shot（少样本）：给几个例子
┌──────────────────────────────────┐
│ 请将英文翻译成中文。              │ ← 任务描述
│ "Hello" → "你好"                 │ ← 示例 1
│ "Thank you" → "谢谢"            │ ← 示例 2
│ "Good morning" → "早上好"        │ ← 示例 3
│ "The weather is nice today." →   │ ← 输入
└──────────────────────────────────┘
模型输出："今天天气很好。"
```

**关键发现：** 模型越大，few-shot 的优势越明显。小模型几乎无法做 zero-shot，但 GPT-3 175B 在很多任务上仅靠 zero-shot 就能达到不错的效果。

### 3. In-Context Learning (ICL)：不需要微调就能学新任务

这是 GPT-3 最神奇也最令人费解的能力：

> **比喻：考场上的"临场学习"**

传统微调就像考前复习——提前花大量时间学习和练题。而 ICL 像是在考场上，看了卷子开头的几个例题和答案，就立刻"悟了"出题套路，然后做出后面的题目。

```
ICL 的神奇之处：

  传统微调：
  训练数据 → 更新模型参数 → 部署 → 推理
  （需要梯度下降，修改权重，很慢）

  In-Context Learning：
  示例放入提示词 → 直接推理
  （模型参数完全不变！只是"读懂"了上下文）

  这意味着：
  ┌─────────────────────────────────────────┐
  │ 同一个模型，换一下提示词，               │
  │ 就能变成翻译器、写作助手、代码生成器、    │
  │ 情感分析器、问答系统……                   │
  │                                         │
  │ 一个模型 = 无数个专用模型！               │
  └─────────────────────────────────────────┘
```

为什么 ICL 能工作？至今仍是一个活跃的研究课题。一种理论认为，模型在预训练时隐式地学会了"学习算法"——它见过太多"给定上下文→生成对应内容"的模式，以至于它学会了根据上下文来调整自己的行为。

### 4. 涌现能力 (Emergent Abilities)

GPT-3 的某些能力不是渐进出现的，而是在模型达到一定规模后"突然出现"的：

```
涌现能力示意：

任务准确率 ↑
  │
  │                        ╭── 突然开窍！
  │                      ╱
  │                    ╱
  │  ..................     ← 小模型：基本不会
  │  .
  │  .
  └──.──────────────────────→ 模型规模
     小                  大

例子：
  - 三位数加法：小模型完全随机猜，GPT-3 175B 突然能算对
  - 理解讽刺：小模型无法识别反话，大模型突然能理解
  - 多步推理：需要一定规模才能做链式推理
```

这一发现有着深远的影响——它意味着我们可能无法通过小模型预测大模型会有什么新能力，**未来的更大模型可能展现出我们完全想不到的能力**。

### 5. GPT-3 的架构

GPT-3 的架构本身并不新颖——它就是一个放大版的 Transformer Decoder：

```
GPT-3 架构细节：

  ┌──────────────────────────────────┐
  │        GPT-3 175B               │
  │                                  │
  │  层数:         96 层             │
  │  隐藏维度:     12288             │
  │  注意力头数:   96 个             │
  │  上下文长度:   2048 tokens       │
  │  词表大小:     50257             │
  │  参数总量:     175,000,000,000   │
  │                                  │
  │  对比 GPT-2:                     │
  │  层数 96 vs 48                   │
  │  维度 12288 vs 1600              │
  │  参数 175B vs 1.5B (117倍)       │
  └──────────────────────────────────┘

  创新不在架构，在于：
  1. 规模空前（大力出奇迹）
  2. 发现了 ICL 这个新范式
  3. 系统化评估了 few-shot 能力
```

## 关键公式解读

GPT-3 作为自回归语言模型，核心目标函数很简单：

$$\mathcal{L} = -\sum_{i=1}^{N} \log P(x_i \mid x_1, x_2, \ldots, x_{i-1}; \theta)$$

| 部分 | 含义 | 直觉 |
|------|------|------|
| $x_i$ | 序列中第 i 个 token | 当前要预测的词 |
| $x_1, \ldots, x_{i-1}$ | 之前的所有 token | 上文语境 |
| $P(\cdot)$ | 模型预测的概率 | 模型认为下一个词是什么的信心 |
| $\theta$ | 模型参数（1750亿个） | 模型的"知识" |

目标：最大化模型根据上文正确预测下一个词的概率。就是这么简单的目标，当规模足够大时，催生了令人惊叹的智能。

## 实验结果

GPT-3 在几十个 NLP 基准任务上进行了系统评估：

| 任务类别 | 代表性任务 | Few-shot 表现 | vs 微调 SOTA |
|---------|----------|--------------|-------------|
| 翻译 | 英→法 | 32.6 BLEU | 接近微调水平 |
| 问答 | TriviaQA | 71.2% | **超越**微调 |
| 阅读理解 | SuperGLUE | 71.8 | 接近人类 |
| 常识推理 | PIQA | 82.8% | 接近微调 |
| 算术 | 两位数加法 | 100% | — |
| 算术 | 三位数加法 | 80% | — |
| 新词理解 | 造词定义 | 高质量 | 无法对比 |
| 文章生成 | 新闻写作 | 人类难辨真假 | — |

特别亮点：
- **TriviaQA 问答**：few-shot GPT-3 超越了所有微调模型
- **新闻生成**：人类评估者只有 52% 的概率能分辨 GPT-3 写的新闻和真实新闻（接近随机猜）
- **SAT 类比题**：few-shot 准确率达到大学申请者平均水平

## 后续影响

GPT-3 的影响怎么评估都不为过——它直接开启了大语言模型时代：

```
GPT-3 (2020)
    │
    ├── 直接后续
    │   ├── Codex (2021) — 代码生成（GitHub Copilot 的基础）
    │   ├── InstructGPT (2022) — RLHF 对齐人类偏好
    │   ├── ChatGPT (2022.11) — 对话式 AI，全球爆火
    │   ├── GPT-4 (2023) — 多模态，更强大
    │   └── GPT-4o (2024) — 原生多模态
    │
    ├── 竞争者涌现
    │   ├── PaLM / Gemini (Google)
    │   ├── Claude (Anthropic)
    │   ├── LLaMA (Meta, 开源)
    │   ├── 通义千问 (阿里)
    │   ├── 文心一言 (百度)
    │   └── DeepSeek (幻方)
    │
    ├── 催生的新范式
    │   ├── Prompt Engineering — 提示词工程成为新技能
    │   ├── Chain-of-Thought — 思维链推理
    │   ├── RLHF — 人类反馈强化学习
    │   └── RAG — 检索增强生成
    │
    └── 产业影响
        ├── AI 创业潮（千亿美元级投资涌入）
        ├── "百模大战"（中国、美国、欧洲）
        └── AI 安全与对齐成为核心议题
```

### GPT-3 与 ChatGPT 的关系

很多人混淆 GPT-3 和 ChatGPT。简单来说：

```
GPT-3 (2020) ─ 纯语言模型，只会"续写"
    │
    ↓ + 人类反馈强化学习 (RLHF)
    ↓ + 指令微调 (Instruction Tuning)
    ↓ + 对话格式优化
    │
InstructGPT / ChatGPT (2022) ─ 会"对话"，懂"指令"

GPT-3 是"原始智力"，ChatGPT 是"受过教育的智力"
```

## 推荐阅读顺序

读这篇论文前，建议先了解：

1. [[Research/AI-经典论文库/01-Attention-Is-All-You-Need|Transformer]] — GPT-3 的骨架
2. **GPT-1 (2018)** — 理解生成式预训练的起源
3. **GPT-2 (2019)** — 理解规模扩大的初步效果
4. [[Research/AI-经典论文库/03-BERT|BERT]] — 理解"预训练+微调"范式（GPT-3 要突破的）
5. **Scaling Laws for Neural LMs (2020)** — 理论基础

读完这篇之后，建议继续阅读：
- **InstructGPT (2022)** — 理解 RLHF 如何让模型"听话"
- **Chain-of-Thought Prompting (2022)** — 让大模型"思考"的方法
- **LLaMA (2023)** — 开源大模型的里程碑

## 参考资源

- **李沐精读系列**：[GPT-3 论文逐段精读](https://www.bilibili.com/video/BV1AF411b7xQ/) — 最推荐的中文讲解
- **OpenAI 官方博客**：[Language Models are Few-Shot Learners](https://openai.com/research/language-models-are-few-shot-learners) — 第一手解读
- **Yannic Kilcher**：[GPT-3 Paper Review](https://www.youtube.com/watch?v=SY5PvZrJhLE) — 深入的英文讲解
- **State of GPT (Karpathy)**：[Microsoft Build 2023 演讲](https://www.youtube.com/watch?v=bZQun8Y4L2A) — 理解 GPT 系列全貌

---
## 相关笔记
- [[Research/AI-从零开始完整学习指南.md|AI 从零开始完整学习指南]]
- [[Research/AI-经典论文库/00-论文索引.md|经典论文索引]]
